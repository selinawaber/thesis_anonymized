{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling a value investor’s stock selection using machine learning\n",
    "## Thesis by Selina Waber, November 2023\n",
    "\n",
    "This code is extracted from my thesis, with all confidential information carefully excluded. Recognizing the sensitivity of financial data, I have omitted any references to such details. As a result, many functions may not execute properly after removal. This repository is intended solely for reading purposes, providing an anonymized version to showcase  my Python skills.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.gridspec as gridspec\n",
    "import scipy.stats as stats\n",
    "from sklearn import metrics\n",
    "from sklearn.base import clone\n",
    "import shap\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from scipy.stats import chi2_contingency, f_oneway\n",
    "from scipy.stats import zscore\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "\n",
    "\n",
    "\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import loguniform\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "\n",
    "## not used?\n",
    "from sklearn.calibration import LabelEncoder\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Ignore Filterwarnings ##############\n",
    "# to avoid \"UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\" spam\n",
    "# for models where some element(s) in the confusion matrix is/are 0\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OurDataset()\n",
    "\n",
    "The dataset was provided by the company as an .xlsx Excel file, listing all the stock sales and buys over a given period of time while listing 15 financial indicators. \n",
    "\n",
    "We define a class `OurDataset()` to handle all stages of pre-processing and missing data imputation, outlier cleaning and train/validation/test split. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurDataset():\n",
    "    def __init__(self, path=None):\n",
    "        self.X: pd.DataFrame = None\n",
    "        self.y: pd.DataFrame = None\n",
    "        self.data: pd.DataFrame = None\n",
    "        if path is not None:\n",
    "            self.load_dataset(path)                  \n",
    "        self.X_train: pd.DataFrame = None\n",
    "        self.X_test: pd.DataFrame = None\n",
    "        self.y_train: pd.DataFrame = None\n",
    "        self.y_test: pd.DataFrame = None\n",
    "        self.X_val: pd.DataFrame = None\n",
    "        self.y_val: pd.DataFrame = None\n",
    "\n",
    "        self.X_train_not_imputed = None\n",
    "        self.X_val_not_imputed = None\n",
    "        self.X_test_not_imputed = None\n",
    "\n",
    "    def set_data(self, dataset):\n",
    "        self.data = dataset\n",
    "        self.y = self.data['Target'] \n",
    "        self.X = self.data.drop( ['Date',  'Company Name', 'Bloomberg Ticker',\n",
    "                             'ISIN', 'CIQ ISIN', 'Acquired', 'Target'], axis = 1)\n",
    "        \n",
    "\n",
    "    def load_dataset(self, project_directory):\n",
    "        data_directory = os.path.join(project_directory, 'Data')\n",
    "        ## Have to do some formating, since column names/ header starts at second row\n",
    "        xls = pd.ExcelFile(os.path.join(data_directory, f'data.xlsx'))\n",
    "        df_bought = pd.read_excel(xls, 'Käufe', skiprows=1)               \n",
    "        df_sold = pd.read_excel(xls, 'Verk', skiprows=1)\n",
    "   \n",
    "        \n",
    "        ### Add the Column 'Acquired' and creating the Target class buy=1 and sell=0.\n",
    "        df_bought['Acquired']= 'buy' ## Adding Columns\n",
    "        df_bought['Target'] = 1\n",
    "\n",
    "        df_sold['Acquired'] = 'sell' ## Adding Columns\n",
    "        df_sold['Target'] = 0\n",
    "\n",
    "        # Merging the two datasets df_sold and df_bought together.\n",
    "        df = pd.concat([df_bought, df_sold], axis=0, ignore_index=True) # Concat the two datasets\n",
    "\n",
    "        df['Acquired'] = df['Acquired'].astype('category') # Converting type of columns to category\n",
    "        df['Target'] = df['Target'].astype('category') ## convert 'Target' to categorical data\n",
    "        \n",
    "\n",
    "        # Convert from object to float, and convert non numbers to NA in floats\n",
    "        list_original_indicators = ['Financial_Ratio_1','Financial_Ratio_2', 'Financial_Ratio_3', 'Financial_Ratio_4',\n",
    "       'Financial_Ratio_5', 'Financial_Ratio_6', 'Financial_Ratio_7', 'Financial_Ratio_8',\n",
    "       'Financial_Ratio_9', 'Financial_Ratio_10',\n",
    "       'Financial_Ratio_11', 'Financial_Ratio_12',\n",
    "       'Financial_Ratio_13', 'Financial_Ratio_14']\n",
    "\n",
    "        for col in list_original_indicators:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce') #If ‘coerce’, then invalid parsing will be set as NaN.\n",
    "       \n",
    "        self.set_data(df) #important, this set always adjusts X and y as wells as data!!\n",
    "\n",
    "        ## Print the Shape to check\n",
    "        print(\"Shape Original Dataset:\", df.shape)\n",
    "        print(\"========================================\")\n",
    "        \n",
    "\n",
    "    def remove_timestamps(self):\n",
    "        # Dropping the columns which where only used by the client for downloading from Bloomberg\n",
    "        self.set_data( self.data.drop(columns=['Datum -5J', 'Datum -4J', 'Datum -3J', 'Datum -2J', 'Datum -1J', 'Datum -6m', 'Datum -3m', 'Datum -1m' ]))\n",
    "        print(\"Removing Bloomberg Timestamps, new shape is:\", self.data.shape)\n",
    "        print(\"========================================\")\n",
    "\n",
    "    def remove_duplicates(self):\n",
    "        ## Dropping Row Duplicates since there is no information in which quantities stock were bought/sold\n",
    "        # Select duplicate row based on the indicator columns and target columns, but not on Date! or on the company name since the company name is not always written the same!\n",
    "        subset1 = [ 'Bloomberg Ticker', 'ISIN', 'CIQ ISIN','Financial_Ratio_1','Financial_Ratio_2', 'Financial_Ratio_3', 'Financial_Ratio_3',\n",
    "       'Financial_Ratio_4', 'Financial_Ratio_6', 'Financial_Ratio_7', 'Financial_Ratio_8',\n",
    "       'Financial_Ratio_9', 'Financial_Ratio_10',\n",
    "       'Financial_Ratio_11', 'Financial_Ratio_12',\n",
    "       'Financial_Ratio_13', 'Financial_Ratio_14', 'Acquired', 'Target']\n",
    "        \n",
    "        df_duplicates = self.data[self.data.duplicated(keep=False, subset=subset1)] # keep = False marks all duplicates as Ture\n",
    "        print(\"There are this many row duplicates:\", df_duplicates.shape[0])\n",
    "        #print(df_duplicates[['Date', 'Company Name']])\n",
    "        df_duplicates.to_excel(\"duplicates.xlsx\", index=True) \n",
    "        #### Should I adjust the index???\n",
    "        self.set_data(self.data.drop_duplicates(subset=subset1, keep=\"first\"))\n",
    "        print(\"Removing Duplicated Rows, new Shape is:\", self.data.shape)\n",
    "        print(\"========================================\")\n",
    "\n",
    "    def sort_after_date(self):\n",
    "        ## Dates are formated in a mess.....\n",
    "        for i, dateval in enumerate(self.data.Date):\n",
    "            try:\n",
    "                pd.to_datetime(dateval, format='mixed', dayfirst=True)\n",
    "            except:\n",
    "                #print(dateval)\n",
    "                new_dateval = str(dateval).replace(',', '.').replace('..', '.') # attempt to correct the format in a loop\n",
    "                self.data.replace(to_replace=dateval, value=new_dateval, inplace=True)\n",
    "\n",
    "        # Format Dates manually...\n",
    "                     \n",
    "\n",
    "        # Finally convert to datetime and sort the dataset!\n",
    "        self.data['Date'] = pd.to_datetime(self.data['Date'], format=\"mixed\", dayfirst= True)# convert to datetime \n",
    "\n",
    "        # Add unique seconds within each group of identical dates\n",
    "        #self.data['Date'] = self.data.groupby('Date').cumcount().apply(lambda x: pd.to_timedelta(x, unit='s')) + self.data['Date']\n",
    "\n",
    "        ## Finally sorting the dataframe\n",
    "        self.set_data(self.data.sort_values(by='Date').reset_index(drop=True)) # df gets sorted in ascending order of dates, Reset the Index here!!\n",
    "        print(\"========================================\")\n",
    "        print(\"Dataset sorted after Date, Index is resetted\")\n",
    "        print(\"========================================\")\n",
    "        self.data.to_excel(\"sorted_output.xlsx\")\n",
    "\n",
    "    def split_dataset(self, imputer, remove_outliers, shuffle):\n",
    "        if not imputer in [\"drop_all\", \"mean\", \"median\", \"KNN\", \"iterative\", \"keep_missing\", None]:\n",
    "            raise Exception(f\"Invalid imputer type `{imputer}`\")\n",
    "        \n",
    "        ## Train-Test Split\n",
    "        if shuffle == False:\n",
    "            self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, stratify = None, test_size = 0.2, shuffle=False, random_state = 42) \n",
    "            self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(self.X_train, self.y_train, stratify= None, test_size = 0.2, shuffle =False, random_state = 42) \n",
    "            print(\"Train,Validation and Test Split done\") \n",
    "            self.plot_missing_data(self.X_train, self.X_val, self.X_test)\n",
    "            self.plot_class_distribution(self.y_train, self.y_val, self.y_test)\n",
    "            self.print_start_end_time(self.y_train, self.y_val, self.y_test)\n",
    "            ## Can I do this like that?\n",
    "            self.X_train_not_imputed = self.X_train\n",
    "            self.X_val_not_imputed = self.X_val\n",
    "            self.X_test_not_imputed = self.X_test\n",
    "\n",
    "        elif shuffle == True:\n",
    "            self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y,  test_size = 0.2, shuffle=True, stratify = self.y, random_state = 42) \n",
    "            self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(self.X_train, self.y_train, test_size = 0.2, shuffle =True, stratify = self.y_train, random_state = 42) \n",
    "            print(\"Train,Validation and Test Split done\") \n",
    "            self.plot_missing_data(self.X_train, self.X_val, self.X_test)\n",
    "            self.plot_class_distribution(self.y_train, self.y_val, self.y_test)\n",
    "            print(\"========================================\")\n",
    "            ## Can I do this like that?\n",
    "            self.X_train_not_imputed = self.X_train\n",
    "            self.X_val_not_imputed = self.X_val\n",
    "            self.X_test_not_imputed = self.X_test\n",
    "\n",
    "        else:\n",
    "            raise Exception(f\"Invalid Shuffle `{shuffle}`\")\n",
    "\n",
    "\n",
    "        if imputer != None:\n",
    "            self.impute_missing_data(imputer)\n",
    "            #In order to remove outliers with IsolationForest, missing values have to be imputed or dropped\n",
    "            if remove_outliers: # maybe it would make more sense to first remove outliers then impute but IsolationForest does not accecpt NAN!\n",
    "                if imputer==\"keep_missing\":\n",
    "                    raise Exception(f\"Removing outliers is incompatible with option `{imputer}`\")\n",
    "                self.remove_outliers()\n",
    "        \n",
    "    \n",
    "    def remove_outliers(self):\n",
    "        # Isolation Forest\n",
    "        outliers = IsolationForest(random_state = 42, contamination = 0.05).fit(self.X_train) # fit Isolation Forest only to training data\n",
    "        inliers_train = outliers.predict(self.X_train)\n",
    "        inliers_test = outliers.predict(self.X_test)\n",
    "        inliers_val = outliers.predict(self.X_val)\n",
    "\n",
    "        # Remove outliers where 1 represent inliers and -1 represent outliers:\n",
    "        self.X_train = self.X_train[np.where(inliers_train == 1, True, False)]\n",
    "        self.y_train = self.y_train[np.where(inliers_train == 1, True, False)]\n",
    "        self.X_test = self.X_test[np.where(inliers_test == 1, True, False)]\n",
    "        self.y_test = self.y_test[np.where(inliers_test == 1, True, False)]\n",
    "        self.X_val = self.X_val[np.where(inliers_val == 1, True, False)]\n",
    "        self.y_val = self.y_val[np.where(inliers_val == 1, True, False)]\n",
    "\n",
    "        print(\"========================================\")\n",
    "        print(\"Outliers removed:\")\n",
    "        print(f\"Shape of X_train is {self.X_train.shape}, and y_train is: {len(self.y_train)}\")\n",
    "        print(f\"Shape of X_val is: {self.X_val.shape} and y_val is: {len(self.y_val)}\")\n",
    "        print(f\"Shape of X_test is:{self.X_test.shape} and y_test is: {len(self.y_test)}\")\n",
    "        print(\"========================================\")\n",
    " \n",
    "\n",
    "    def impute_missing_data(self, imputer):\n",
    "\n",
    "        if imputer == \"drop_all\":\n",
    "            # Drop missing values from X_train and adjust y_train\n",
    "            self.X_train = self.X_train.dropna()\n",
    "            self.y_train = self.y_train[self.X_train.index]\n",
    "            # Drop missing values from X_test and adjust y_test\n",
    "            self.X_test = self.X_test.dropna()\n",
    "            self.y_test = self.y_test[self.X_test.index]\n",
    "            # Drop missing values from X_val and adjust y_val\n",
    "            self.X_val = self.X_val.dropna()\n",
    "            self.y_val = self.y_val[self.X_val.index]\n",
    "\n",
    "        else:\n",
    "            df_NA_train = self.X_train.copy(deep=True)\n",
    "            df_NA_val = self.X_val.copy(deep=True)\n",
    "            df_NA_test = self.X_test.copy(deep=True)\n",
    "            \n",
    "            for col in self.X.columns.values:\n",
    "                df_NA_train[f'{col}_NA'] = self.X_train[col].isna().astype(\"category\")  # convert to category\n",
    "                df_NA_test[f'{col}_NA'] = self.X_test[col].isna().astype(\"category\")  # convert to category\n",
    "                df_NA_val[f'{col}_NA'] = self.X_val[col].isna().astype(\"category\")  # convert to category\n",
    "\n",
    "            if imputer ==\"keep_missing\":\n",
    "                self.X_train = self.X_train\n",
    "                self.y_train = self.y_train\n",
    "                self.y_val = self.y_val\n",
    "\n",
    "            elif imputer == \"median\":\n",
    "                imputer = SimpleImputer(strategy='median')\n",
    "                self.X_train = pd.DataFrame(imputer.fit_transform(self.X_train), columns=imputer.get_feature_names_out(self.X_train.columns)) \n",
    "                self.X_test = pd.DataFrame(imputer.transform(self.X_test), columns = imputer.get_feature_names_out(self.X_test.columns))\n",
    "                self.X_val = pd.DataFrame(imputer.transform(self.X_val), columns = imputer.get_feature_names_out(self.X_val.columns))\n",
    "            \n",
    "            elif imputer == \"mean\":\n",
    "                imputer = SimpleImputer(strategy='mean')\n",
    "                self.X_train = pd.DataFrame(imputer.fit_transform(self.X_train), columns=imputer.get_feature_names_out(self.X_train.columns)) #auf Train fit_transform\n",
    "                self.X_test = pd.DataFrame(imputer.transform(self.X_test), columns = imputer.get_feature_names_out(self.X_test.columns)) # auf test nur transform \n",
    "                self.X_val = pd.DataFrame(imputer.transform(self.X_val), columns = imputer.get_feature_names_out(self.X_val.columns)) # auf val nur transform \n",
    "    \n",
    "            elif imputer == \"KNN\":\n",
    "                print(\"Attention, for KNN: X_train and X_test will be sacled with StandardScaler, but I inverse transform the scale back\")\n",
    "                scaler = StandardScaler() # or MinMaxScaler()?\n",
    "                self.X_train = pd.DataFrame(scaler.fit_transform(self.X_train), columns = self.X_train.columns)\n",
    "                self.X_test = pd.DataFrame(scaler.transform(self.X_test), columns = self.X_test.columns)\n",
    "                self.X_val = pd.DataFrame(scaler.transform(self.X_val), columns = self.X_val.columns)\n",
    "\n",
    "                imputer = KNNImputer(n_neighbors = 5, weights = \"distance\")\n",
    "                self.X_train = pd.DataFrame(imputer.fit_transform(self.X_train), columns=imputer.get_feature_names_out(self.X_train.columns))\n",
    "                self.X_test = pd.DataFrame(imputer.transform(self.X_test), columns=imputer.get_feature_names_out(self.X_test.columns))\n",
    "                self.X_val = pd.DataFrame(imputer.transform(self.X_val), columns=imputer.get_feature_names_out(self.X_val.columns))\n",
    "\n",
    "                # If needed, inverse transform to get the data back to its original scale\n",
    "                self.X_train = pd.DataFrame(scaler.inverse_transform(self.X_train), columns=imputer.get_feature_names_out(self.X_train.columns))\n",
    "                self.X_test = pd.DataFrame(scaler.inverse_transform(self.X_test), columns=imputer.get_feature_names_out(self.X_test.columns))\n",
    "                self.X_val = pd.DataFrame(scaler.inverse_transform(self.X_val), columns=imputer.get_feature_names_out(self.X_val.columns))\n",
    "            \n",
    "            elif imputer == \"iterative\":\n",
    "                # Initialize the imputer\n",
    "                imputer = IterativeImputer(max_iter=10, random_state=42)\n",
    "                # Fit on the training data and transform both train and test data\n",
    "                self.X_train = pd.DataFrame(imputer.fit_transform(self.X_train), columns=imputer.get_feature_names_out(self.X_train.columns)) #auf Train fit_transform\n",
    "                self.X_test = pd.DataFrame(imputer.transform(self.X_test), columns = imputer.get_feature_names_out(self.X_test.columns)) # auf test nur transform \n",
    "                self.X_val = pd.DataFrame(imputer.transform(self.X_val), columns = imputer.get_feature_names_out(self.X_val.columns)) # auf val nur transform \n",
    "\n",
    "            else:\n",
    "                raise Exception(f'Invalid imputer type \"{imputer}\"')\n",
    "         \n",
    "        #Target as category\n",
    "        self.y_train = self.y_train.astype(\"category\")\n",
    "        self.y_test = self.y_test.astype(\"category\")\n",
    "        self.y_val = self.y_val.astype(\"category\")\n",
    "        \n",
    "        #Rest the index!!!\n",
    "        self.X_train = self.X_train.reset_index(drop=True)\n",
    "        self.y_train = self.y_train.reset_index(drop=True)\n",
    "        self.X_test = self.X_test.reset_index(drop= True)\n",
    "        self.y_test = self.y_test.reset_index(drop=True)\n",
    "        self.X_val = self.X_val.reset_index(drop= True)\n",
    "        self.y_val = self.y_val.reset_index(drop=True)\n",
    "        print(\"Index Adjusted!\")\n",
    "\n",
    "        # Add the indicator variables back to the dataframe\n",
    "        if imputer != \"drop_all\":\n",
    "            for col in df_NA_train.columns.values:\n",
    "                if str(col).endswith(\"_NA\"):\n",
    "                    self.X_train[col] = df_NA_train[col].reset_index(drop=True)\n",
    "                    self.X_test[col] = df_NA_test[col].reset_index(drop=True)\n",
    "                    self.X_val[col] = df_NA_val[col].reset_index(drop=True)\n",
    "        \n",
    "        print(\"========================================\")\n",
    "        print(\"Missing Values Imputed, new Shape is:\")\n",
    "        print(f\"Shape of X_train is {self.X_train.shape}, and y_train is: {len(self.y_train)}\")\n",
    "        print(f\"Shape of X_val is: {self.X_val.shape} and y_val is: {len(self.y_val)}\")\n",
    "        print(f\"Shape of X_test is:{self.X_test.shape} and y_test is: {len(self.y_test)}\")\n",
    "        print(\"========================================\")\n",
    "        \n",
    "\n",
    "    def convert_to_categorical(self, list_features):\n",
    "        #categorical_features = self.X.select_dtypes(include='int64').columns.tolist()\n",
    "        for feature in list_features:\n",
    "            self.X[feature] = self.X[feature].astype('category')\n",
    "\n",
    "    def _get_group(self, year):\n",
    "        return self.data.groupby(self.data.Date.dt.year).get_group(year)\n",
    "    \n",
    "    def get_year_dataset(self, year):\n",
    "        year_df = OurDataset()\n",
    "        year_df.set_data(self._get_group(year))\n",
    "        return year_df\n",
    "    \n",
    "    def drop_year_dataset(self, year):\n",
    "        new_ds = OurDataset()\n",
    "        new_ds.set_data(self.data[self.data[\"Date\"].dt.year != year])\n",
    "        return new_ds\n",
    "        \n",
    "    def list_years(self):\n",
    "        return list(self.data['Date'].dt.year.unique())\n",
    "    \n",
    "    def remove_columns(self, columns):\n",
    "        self.set_data(self.data.drop(columns=columns, axis = 1))\n",
    "        \n",
    "    def clean_multiproducts(self):\n",
    "        idx_to_remove = ~self.data['Bloomberg Ticker'].str.contains(\"XXXXXX\", case=False, na=False) # anonymized\n",
    "        idx_to_remove |= self.data['Company Name'].str.contains(\"XXXXXX\", case=False, na=False) # anonymized\n",
    "        idx_to_remove |= self.data['Company Name'].str.contains(\"XXXXXX\", case=False, na=False) # anonymized\n",
    "        idx_to_remove |= self.data['Company Name'].str.contains(\"XXXXXX\", case=False, na=False) # anonymized\n",
    "        idx_to_remove |= self.data['Company Name'].str.contains(\"XXXXXX\", case=False, na=False) # anonymized\n",
    "\n",
    "        multiproduct_df = self.data.loc[idx_to_remove]\n",
    "        multiproduct_df.to_excel(\"multiproduct.xlsx\")\n",
    "        self.set_data(self.data[~idx_to_remove])\n",
    "        print(\"========================================\")\n",
    "        print(\"Multiproducts are removed, only Stocks are left, new shape is: \", self.data.shape)\n",
    "        print(\"========================================\")\n",
    "\n",
    "    def remove_stocks_with_lacking_data(self, min_datapoints=2):\n",
    "        indicators = self.X.columns\n",
    "        idx_to_remove = self.data[indicators].isna().sum(axis=1).ge(len(indicators.to_list()) - min_datapoints + 1)\n",
    "        single_indicator_df = self.data[idx_to_remove]\n",
    "        single_indicator_df.to_excel(\"single_indicator.xlsx\")\n",
    "        self.set_data(self.data[~idx_to_remove])\n",
    "        print(\"========================================\")\n",
    "        print(f\"Stocks with less than {min_datapoints} indicators\", len(single_indicator_df))\n",
    "        print(f\"Stock with less than {min_datapoints} are removed, new shape is:\", self.data.shape)\n",
    "        print(\"========================================\")\n",
    "\n",
    "    def plot_missing_data(self, X_train, X_val, X_test):\n",
    "        # Calculate total data points and missing data points for each set\n",
    "        total_train = X_train.size\n",
    "        missing_train = X_train.isnull().sum().sum()\n",
    "        na_percentage_total_train = (X_train.isna().sum().sum() / X_train.size) * 100\n",
    "        print(\"Missing Data for X_train:\", na_percentage_total_train)\n",
    "\n",
    "        total_val = X_val.size\n",
    "        missing_val = X_val.isnull().sum().sum()\n",
    "        na_percentage_total_val = (X_val.isna().sum().sum() / X_val.size) * 100\n",
    "        print(\"Missing Data for X_val:\", na_percentage_total_val)\n",
    "        \n",
    "        total_test = X_test.size\n",
    "        missing_test = X_test.isnull().sum().sum()\n",
    "        na_percentage_total_test = (X_test.isna().sum().sum() / X_test.size) * 100\n",
    "        print(\"Missing Data for X_test:\", na_percentage_total_test)\n",
    "        \n",
    "        # Data for plotting\n",
    "        datasets = ['Training', 'Validation', 'Testing']\n",
    "        total_data = [total_train, total_val, total_test]\n",
    "        missing_data = [missing_train, missing_val, missing_test]\n",
    "        \n",
    "        barWidth = 0.3\n",
    "        \n",
    "        # Set position of bar on X axis\n",
    "        r1 = np.arange(len(total_data))\n",
    "        r2 = [x + barWidth for x in r1]\n",
    "        \n",
    "        # Make the plot\n",
    "        plt.bar(r1, total_data, color='blue', width=barWidth, edgecolor='grey', label='Total Data Points')\n",
    "        plt.bar(r2, missing_data, color='red', width=barWidth, edgecolor='grey', label='Missing Data Points')\n",
    "        \n",
    "        # Adding labels\n",
    "        plt.xlabel('Datasets', fontweight='bold', fontsize=15)\n",
    "        plt.ylabel('Count', fontweight='bold', fontsize=15)\n",
    "        plt.xticks([r + barWidth/2 for r in range(len(total_data))], datasets)\n",
    "        \n",
    "        # Create legend & Show graphic\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_class_distribution(self, y_train, y_val, y_test):\n",
    "        # Calculate class distribution for each set\n",
    "        class_0_train = (y_train == 0).sum()\n",
    "        class_1_train = (y_train == 1).sum()\n",
    "        \n",
    "        class_0_val = (y_val == 0).sum()\n",
    "        class_1_val = (y_val == 1).sum()\n",
    "        \n",
    "        class_0_test = (y_test == 0).sum()\n",
    "        class_1_test = (y_test == 1).sum()\n",
    "\n",
    "        print(\"y_train:\" , y_train.value_counts())\n",
    "        class_counts_normalized_train = y_train.value_counts(normalize=True)\n",
    "        print(\"y_train\", class_counts_normalized_train)\n",
    "        print(\"y_val:\", y_val.value_counts())\n",
    "        class_counts_normalized_val = y_val.value_counts(normalize=True)\n",
    "        print(\"y_val: \", class_counts_normalized_val)\n",
    "        print(\"y_test:\" , y_test.value_counts())\n",
    "        class_counts_normalized_test = y_test.value_counts(normalize=True)\n",
    "        print(\"y_test: \", class_counts_normalized_test)\n",
    "        \n",
    "        # Data for plotting\n",
    "        datasets = ['Training', 'Validation', 'Testing']\n",
    "        class_0 = [class_0_train, class_0_val, class_0_test]\n",
    "        class_1 = [class_1_train, class_1_val, class_1_test]\n",
    "        \n",
    "        barWidth = 0.3\n",
    "        \n",
    "        # Set position of bar on X axis\n",
    "        r1 = np.arange(len(class_0))\n",
    "        r2 = [x + barWidth for x in r1]\n",
    "        \n",
    "        # Make the plot\n",
    "        plt.bar(r1, class_0, color='blue', width=barWidth, edgecolor='grey', label='Class Sell=0')\n",
    "        plt.bar(r2, class_1, color='orange', width=barWidth, edgecolor='grey', label='Class Buy=1')\n",
    "        \n",
    "        # Adding labels\n",
    "        plt.xlabel('Datasets', fontweight='bold', fontsize=15)\n",
    "        plt.ylabel('Count', fontweight='bold', fontsize=15)\n",
    "        plt.xticks([r + barWidth/2 for r in range(len(class_0))], datasets)\n",
    "        \n",
    "        # Create legend & Show graphic\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def print_start_end_time(self, X_train, X_val, X_test):\n",
    "            same_index_train = self.data.merge(X_train, how=\"inner\", left_index=True, right_index=True)\n",
    "            same_index_val = self.data.merge(X_val, how=\"inner\", left_index=True, right_index=True)\n",
    "            same_index_test = self.data.merge(X_test, how=\"inner\", left_index=True, right_index=True)\n",
    "            print(f\"X_train: First date: {same_index_train['Date'].min()}, Last date: {same_index_train['Date'].max()}, X_train: {X_train.shape[0]}\")\n",
    "            print(f\"X_val: First date: {same_index_val['Date'].min()}, Last date: {same_index_val['Date'].max()}, X_val: {X_val.shape[0]}\")\n",
    "            print(f\"X_test: First date: {same_index_test['Date'].min()}, Last date: {same_index_test['Date'].max()}, X_test: {X_test.shape[0]}\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Time-Series Tests and Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seasonal Decomposition**:\n",
    "### Could not implement seasonal_decomposition since Date is has duplicates since only day/month/year are given and no minutes/hour/seconds\n",
    "\n",
    "## Time-Series-Plot:\n",
    "def plot_time_series(X):\n",
    "    num_plots = X.shape[1]\n",
    "    cols = int(np.ceil(np.sqrt(num_plots)))\n",
    "    rows = int(np.ceil(num_plots / cols))\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 15))\n",
    "\n",
    "    for idx, col in enumerate(X.columns):\n",
    "        i, j = divmod(idx, cols)\n",
    "        X[col].plot(ax=axes[i, j], title=f'Time Series for {col}')\n",
    "        axes[i, j].set_ylabel(col)\n",
    "        axes[i, j].set_xlabel('Time')\n",
    "        axes[i, j].grid(True)\n",
    "    \n",
    "    # Remove extra subplots\n",
    "    for idx in range(num_plots, rows * cols):\n",
    "        i, j = divmod(idx, cols)\n",
    "        fig.delaxes(axes[i, j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#Autocorrelation and Partial Autocorrelation Plots\n",
    "def plot_autocorrelation(X):\n",
    "    rows = X.shape[1]\n",
    "    fig, axes = plt.subplots(rows, 2, figsize=(15, 60))\n",
    "    \n",
    "    for idx, col in enumerate(X.columns):\n",
    "        plot_acf(X[col], ax=axes[idx, 0])\n",
    "        plot_pacf(X[col], ax=axes[idx, 1])\n",
    "        \n",
    "        axes[idx, 0].set_title(f'ACF for {col}')\n",
    "        axes[idx, 1].set_title(f'PACF for {col}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Dickey-Fuller Test. This is a statistical test to check the stationarity of a time series.\n",
    "def test_stationarity(X_clean):\n",
    "    for col in X_clean.columns:\n",
    "        print(f\"Results of Dickey-Fuller Test for {col}:\")\n",
    "        dftest = adfuller(X_clean[col], autolag='AIC')\n",
    "        dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])\n",
    "        for key, value in dftest[4].items():\n",
    "            dfoutput[f'Critical Value ({key})'] = value\n",
    "        print(dfoutput.apply(lambda x: \"{0:.6f}\".format(x) if abs(x) > 0.000001 else \"{:e}\".format(x)))\n",
    "        print('---------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_missing_data_heatmap(df):\n",
    "    \n",
    "    X = df.drop(['Date', 'Company Name', 'Bloomberg Ticker',\n",
    "                 'ISIN', 'CIQ ISIN', 'Acquired', 'Target'], axis=1)\n",
    "\n",
    "    # Set Seaborn styling\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.style.use('seaborn-notebook')\n",
    "\n",
    "    # Heatmap visualization\n",
    "    plt.figure(figsize=(25, 10))\n",
    "    # Extract unique years from the 'Date' column\n",
    "    unique_years = df['Date'].dt.year.unique()\n",
    "    # Generate ticks positions corresponding to the start of each unique year\n",
    "    ticks_positions = [df[df['Date'].dt.year == year].index[0] for year in unique_years]\n",
    "    # Create the heatmap\n",
    "    sns.heatmap(X.isnull().transpose(), cmap='YlGnBu', cbar_kws={'label': 'Missing Values'}, xticklabels=False)\n",
    "    # Set x-axis labels and ticks\n",
    "    plt.xticks(ticks_positions, unique_years)\n",
    "    plt.xlabel('Years')\n",
    "    plt.title(\"Heatmap of Missing Values\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_missing_data_tabel(X):\n",
    "\n",
    "    print(f'There are in total {X.isnull().sum().sum()} NAN in the indicator dataframe')\n",
    "    # Percentage calculations\n",
    "    na_percent = X.isna().sum() / X.shape[0] * 100\n",
    "    zero_percent = X.isin([0]).sum() / X.shape[0] * 100\n",
    "    zero_and_na_percent = (X.isna().sum() + X.isin([0]).sum()) / X.shape[0] * 100\n",
    "\n",
    "    # Create missing data summary\n",
    "    missing_data = pd.DataFrame({\n",
    "        'total_NA': X.isna().sum(),\n",
    "        '%_NA': na_percent,\n",
    "        'total_0': X.isin([0]).sum(),\n",
    "        '%_0': zero_percent,\n",
    "        'Total_NA_and_0': (X.isna().sum() + X.isin([0]).sum()),\n",
    "        '%_NA_and_0': zero_and_na_percent\n",
    "    }).sort_values(by='%_NA_and_0', ascending=False)\n",
    "\n",
    "    print(missing_data)\n",
    "\n",
    "    # Bar chart visualization\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(30, 10))\n",
    "\n",
    "    na_labels = na_percent.sort_values(ascending=False).index.values.tolist()\n",
    "    ax[0].bar(na_labels, na_percent.sort_values(ascending=False).values, color='skyblue')\n",
    "    ax[0].set_ylabel(\"NA Percentage\")\n",
    "    ax[0].set_title(\"Features with the highest NA Percentage\")\n",
    "    ax[0].tick_params(axis='x', rotation=90)\n",
    "\n",
    "    ax[1].bar(na_labels, zero_percent.sort_values(ascending=False).values, color='salmon')\n",
    "    ax[1].set_ylabel(\"0 Value Percentage\")\n",
    "    ax[1].set_title(\"Features with the highest 0 Value Percentage\")\n",
    "    ax[1].tick_params(axis='x', rotation=90)\n",
    "\n",
    "    ax[2].bar(na_labels, zero_and_na_percent.sort_values(ascending=False).values, color='lightgreen')\n",
    "    ax[2].set_ylabel(\"NA & 0 Value Percentage\")\n",
    "    ax[2].set_title(\"Features with the highest NA & 0 Value Percentage\")\n",
    "    ax[2].tick_params(axis='x', rotation=90)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_missing_data_simple(X):\n",
    "    # Set Seaborn styling\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.style.use('seaborn-notebook')\n",
    "\n",
    "    # Percentage calculations\n",
    "    na_percent = X.isna().sum() / X.shape[0] * 100\n",
    "\n",
    "    # Bar chart visualization\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    na_labels = na_percent.sort_values(ascending=False).index.values.tolist()\n",
    "    bars = ax.bar(na_labels, na_percent.sort_values(ascending=False).values, color=sns.color_palette(\"coolwarm_r\", len(na_labels)))\n",
    "    \n",
    "    # Adding the exact percentage on top of each bar\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.2f}%',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom',\n",
    "                    fontsize=10)\n",
    "    \n",
    "    ax.set_ylabel(\"NA Percentage\", fontsize=14)\n",
    "    ax.set_title(\"Features with the Highest NA Percentage\", fontsize=16)\n",
    "    ax.tick_params(axis='x', rotation=90, labelsize=12)\n",
    "    ax.tick_params(axis='y', labelsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary PCA Plot of Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_missing_matrix(X):\n",
    "    return X.isna().astype(int)\n",
    "\n",
    "def perform_pca_on_missing_data(binary_matrix, n_components=2):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    principal_components = pca.fit_transform(binary_matrix)\n",
    "    return principal_components, pca  # Return the PCA object as well\n",
    "\n",
    "def plot_pca(principal_components, pca, X, y):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Create a color map based on 'Target' values\n",
    "    colors = ['blue' if val == 0 else 'orange' for val in y]\n",
    "    \n",
    "    plt.scatter(principal_components[:, 0], principal_components[:, 1], alpha=0.5, c=colors)\n",
    "    \n",
    "    # Plot the arrows (loadings)\n",
    "    for i, (comp1, comp2) in enumerate(zip(pca.components_[0], pca.components_[1])):\n",
    "        plt.arrow(0, 0, comp1, comp2, head_width=0.02, head_length=0.03, color='red')\n",
    "        if abs(comp1) > 0.2 or abs(comp2) > 0.2:  # only label arrows that are long enough\n",
    "            plt.text(comp1*1.2, comp2*1.2, X.columns[i], color='black', ha='center', va='center')\n",
    "    \n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.title('PCA of Binary Missing Data Representation\\nExplained Variance: {:.2%} and {:.2%}'.format(pca.explained_variance_ratio_[0], pca.explained_variance_ratio_[1]))\n",
    "    \n",
    "    # Create a legend for the plot\n",
    "    plt.legend(handles=[plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', markersize=10, label='Sell (0)'),\n",
    "                        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='orange', markersize=10, label='Buy (1)')],\n",
    "               loc='upper right')\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_missing_data_pca(X, y):\n",
    "    # 1. Create a binary matrix of missing values\n",
    "    binary_matrix = binary_missing_matrix(X)\n",
    "    \n",
    "    # 2. Perform PCA on the binary matrix\n",
    "    principal_components, pca = perform_pca_on_missing_data(binary_matrix)\n",
    "    \n",
    "    # 3. Plot the results\n",
    "    plot_pca(principal_components, pca, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_missing_matrix(X):\n",
    "    return X.isna().astype(int)\n",
    "\n",
    "def perform_pca_on_missing_data(binary_matrix, n_components=3):  # Default changed to 3 components\n",
    "    pca = PCA(n_components=n_components)\n",
    "    principal_components = pca.fit_transform(binary_matrix)\n",
    "    return principal_components, pca\n",
    "\n",
    "def plot_pca_3d_only_arrows(principal_components, pca, X):\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Plot the arrows (loadings) only\n",
    "    max_length = 0  # This will help set axis limits later\n",
    "    for i, (comp1, comp2, comp3) in enumerate(zip(pca.components_[0], pca.components_[1], pca.components_[2])):\n",
    "        arrow_length = np.sqrt(comp1**2 + comp2**2 + comp3**2)\n",
    "        max_length = max(max_length, arrow_length)\n",
    "        ax.quiver(0, 0, 0, comp1, comp2, comp3, length=arrow_length, arrow_length_ratio=0.1, color='red', linewidth=1.5)\n",
    "        ax.text(comp1*1.2, comp2*1.2, comp3*1.2, X.columns[i], color='black', ha='center', va='center')\n",
    "\n",
    "    # Set axis limits based on the maximum arrow length\n",
    "    ax.set_xlim([-max_length, max_length])\n",
    "    ax.set_ylim([-max_length, max_length])\n",
    "    ax.set_zlim([-max_length, max_length])\n",
    "\n",
    "    ax.set_xlabel('Principal Component 1')\n",
    "    ax.set_ylabel('Principal Component 2')\n",
    "    ax.set_zlabel('Principal Component 3')\n",
    "    ax.set_title('Loadings in PCA Space for Binary Missing Data Representation')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_missing_data_pca_3d_only_arrows(X, y):  # Name changed to signify 3D plotting\n",
    "    binary_matrix = binary_missing_matrix(X)\n",
    "    principal_components, pca = perform_pca_on_missing_data(binary_matrix)\n",
    "    plot_pca_3d_only_arrows(principal_components, pca, X)  # Call 3D plot function\n",
    "\n",
    "\n",
    "def plot_pca_3d(principal_components, pca, X, y):\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    colors = ['blue' if val == 0 else 'orange' for val in y]\n",
    "    ax.scatter(principal_components[:, 0], principal_components[:, 1], principal_components[:, 2], alpha=0.5, c=colors)\n",
    "    \n",
    "    ax.set_xlabel('Principal Component 1')\n",
    "    ax.set_ylabel('Principal Component 2')\n",
    "    ax.set_zlabel('Principal Component 3')\n",
    "    \n",
    "    title = 'PCA of Binary Missing Data Representation\\nExplained Variance: {:.2%}, {:.2%}, and {:.2%}'.format(\n",
    "        pca.explained_variance_ratio_[0], pca.explained_variance_ratio_[1], pca.explained_variance_ratio_[2])\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    # Handling the legend\n",
    "    ax.legend(handles=[plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', markersize=10, label='Sell (0)'),\n",
    "                       plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='orange', markersize=10, label='Buy (1)')],\n",
    "               loc='upper right')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_missing_data_pca_3d(X, y):  # Name changed to signify 3D plotting\n",
    "    binary_matrix = binary_missing_matrix(X)\n",
    "    principal_components, pca = perform_pca_on_missing_data(binary_matrix)\n",
    "    plot_pca_3d(principal_components, pca, X, y)  # Call 3D plot function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalnce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_target_distribution_yearly(df):\n",
    "    X = df.drop( ['Date',  'Company Name', 'Bloomberg Ticker',\n",
    "                             'ISIN', 'CIQ ISIN', 'Acquired', 'Target'], axis = 1)\n",
    "    # Group by year and get unique years\n",
    "    dfgroups = df.groupby(df['Date'].dt.year)\n",
    "    years = df['Date'].dt.year.unique()\n",
    "\n",
    "   # Determine the plotting grid dynamically\n",
    "    num_plots = X.shape[1]\n",
    "    cols = int(np.ceil(np.sqrt(num_plots)))\n",
    "    rows = int(np.ceil(num_plots / cols))\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 15))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for index, year in enumerate(years):\n",
    "        year_df = dfgroups.get_group(year)\n",
    "        sns.barplot(x=year_df['Target'].value_counts().index, \n",
    "                    y=year_df['Target'].value_counts(normalize=True).values, \n",
    "                    ax=axes[index])\n",
    "        axes[index].set_title(year)\n",
    "        axes[index].set_xlabel(\"Target\")\n",
    "        axes[index].set_ylabel(\"Proportion\")\n",
    "        axes[index].set_xticklabels([\"Sell\", \"Buy\"])\n",
    "        axes[index].set_ylim(0, 1)\n",
    "\n",
    "    # Handling any extra subplots\n",
    "    for i in range(len(years), rows * cols):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(\"Buy/Sell Proportions over the Years\", y=1.05)\n",
    "    plt.show()\n",
    "\n",
    "def plot_traget_distribution_simple(y):\n",
    "    # Display counts of target values\n",
    "    print(y.value_counts())\n",
    "    class_counts_normalized = y.value_counts(normalize=True)\n",
    "    print(class_counts_normalized)\n",
    "    # Display skewness and kurtosis\n",
    "    print(\"Skewness: %f\" % y.astype(int).skew()) #does not make sense for binary classification\n",
    "    print(\"Kurtosis: %f\" % y.astype(int).kurt()) #does not make sense for binary classification\n",
    "    # Distribution of target\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.countplot(x=y)\n",
    "    plt.title('Distribution of Target')\n",
    "    plt.xticks(ticks=[0,1], labels=['Sell', 'Buy'])\n",
    "    plt.show()\n",
    "\n",
    "   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms and Distribution for each Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_median_hist(X, y):\n",
    "    num_plots = X.shape[1]\n",
    "    cols = int(np.ceil(np.sqrt(num_plots)))\n",
    "    rows = int(np.ceil(num_plots / cols))\n",
    "    \n",
    "    fig, ax = plt.subplots(rows, cols, figsize=(20, 15))\n",
    "    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "    for i, col in enumerate(X.columns):\n",
    "        j, k = divmod(i, cols)\n",
    "\n",
    "        feat_cl0 = X.loc[y == 0, col]\n",
    "        feat_cl1 = X.loc[y == 1, col]\n",
    "\n",
    "        mean_cl0 = feat_cl0.mean()\n",
    "        mean_cl1 = feat_cl1.mean()\n",
    "\n",
    "        med_cl0 = feat_cl0.median()\n",
    "        med_cl1 = feat_cl1.median() \n",
    "\n",
    "        plot_range = (min(feat_cl0.quantile(0.05), feat_cl1.quantile(0.05)),\n",
    "                      max(feat_cl0.quantile(0.95), feat_cl1.quantile(0.95)))\n",
    "\n",
    "        ax[j, k].hist(feat_cl0, color=\"royalblue\", bins=50, alpha=0.6, range=plot_range, label=\"Sell\")\n",
    "        ax[j, k].hist(feat_cl1, color=\"darkorange\", bins=50, alpha=0.6, range=plot_range, label=\"Buy\")\n",
    "        ax[j, k].axvline(x=mean_cl0, color=\"blue\", linestyle=':', linewidth=2, label=f\"mean {round(mean_cl0, 3)}\")\n",
    "        ax[j, k].axvline(x=med_cl0, color=\"blue\", linestyle='--', linewidth=2, label=f\"median {round(med_cl0, 3)}\")\n",
    "        ax[j, k].axvline(x=mean_cl1, color=\"orange\", linestyle=':', linewidth=2, label=f\"mean {round(mean_cl1, 3)}\")\n",
    "        ax[j, k].axvline(x=med_cl1, color=\"orange\", linestyle='--', linewidth=2, label=f\"median {round(med_cl1, 3)}\")\n",
    "        \n",
    "        ax[j, k].set_xlabel(col, fontsize=12)\n",
    "        ax[j, k].tick_params(axis='x', rotation=45)\n",
    "        ax[j, k].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        ax[j, k].legend(fontsize=10, loc='upper right')\n",
    "\n",
    "    # Remove extra subplots\n",
    "    for l in range(i+1, rows*cols):\n",
    "        j, k = divmod(l, cols)\n",
    "        ax[j, k].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_histogram_scaled_for_KDE(X):\n",
    "    #With this modification, the KDE is plotted on a different y-scale, which should make it more visible and easier to interpret alongside the histogram.\n",
    "    num_plots = X.shape[1]\n",
    "    cols = int(np.ceil(np.sqrt(num_plots)))\n",
    "    rows = int(np.ceil(num_plots / cols))\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 15))\n",
    "\n",
    "    for idx, col in enumerate(X.columns):\n",
    "        i, j = divmod(idx, cols)\n",
    "        \n",
    "        # Histogram without KDE\n",
    "        sns.histplot(X[col], color=\"skyblue\", ax=axes[i, j], label=\"Histogram\")\n",
    "        \n",
    "        # Create a secondary y-axis for the KDE\n",
    "        ax2 = axes[i, j].twinx()\n",
    "        sns.kdeplot(X[col], ax=ax2, color=\"red\", label=\"KDE\")\n",
    "        \n",
    "        # Titles and labels\n",
    "        axes[i, j].set_title(f'Histogram for {col}')\n",
    "        axes[i, j].set_xlabel(col)\n",
    "        axes[i, j].set_ylabel('Count')\n",
    "        ax2.set_ylabel('Density')\n",
    "        axes[i, j].grid(True)\n",
    "\n",
    "    # Remove extra subplots\n",
    "    for idx in range(num_plots, rows * cols):\n",
    "        i, j = divmod(idx, cols)\n",
    "        fig.delaxes(axes[i, j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_histogram(X):\n",
    "    num_plots = X.shape[1]\n",
    "    cols = int(np.ceil(np.sqrt(num_plots)))\n",
    "    rows = int(np.ceil(num_plots / cols))\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 15))\n",
    "\n",
    "    for idx, col in enumerate(X.columns):\n",
    "        i, j = divmod(idx, cols)\n",
    "        \n",
    "        # Histogram with normalization and transparency\n",
    "        sns.histplot(X[col], color=\"skyblue\", ax=axes[i, j], label=\"Histogram\", ) #stat=\"probability\", kde=False\n",
    "        \n",
    "        # Titles and labels\n",
    "        axes[i, j].set_title(f'Histogram for {col}')\n",
    "        axes[i, j].set_xlabel(col)\n",
    "        axes[i, j].set_ylabel('Count') #Density if stats=probablility\n",
    "        axes[i, j].grid(True)\n",
    "        #axes[i, j].legend()\n",
    "\n",
    "    # Remove extra subplots\n",
    "    for idx in range(num_plots, rows * cols):\n",
    "        i, j = divmod(idx, cols)\n",
    "        fig.delaxes(axes[i, j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_hist_yearly(df):\n",
    "    # Filtering out unnecessary columns\n",
    "    X = df.drop(['Date', 'Company Name', 'Bloomberg Ticker', 'ISIN', 'CIQ ISIN', 'Acquired', 'Target'], axis=1)\n",
    "    \n",
    "    dfgroups = df.groupby(df['Date'].dt.year)\n",
    "    years = sorted(df['Date'].dt.year.unique())\n",
    "    topics = list(X.select_dtypes(include=[float]))\n",
    "\n",
    "    for topic in topics:\n",
    "        fig, ax = plt.subplots(nrows=4, ncols=4, figsize=(30, 20))\n",
    "        fig.suptitle(f'{topic} with Kernel Density and Median (red)', fontsize='large')\n",
    "        ax = ax.ravel()  # flatten the axis array\n",
    "\n",
    "        # Plot each year's histogram\n",
    "        for index, year in enumerate(years):\n",
    "            year_df = dfgroups.get_group(year)\n",
    "            ax[index].set_title(year)\n",
    "            sns.histplot(year_df[topic], kde=True, ax=ax[index])\n",
    "            ax[index].axvline(x=year_df[topic].median(), color=\"red\", label=\"Median\", linestyle='-')\n",
    "            \n",
    "            # Plot density of all years together to compare them\n",
    "            sns.kdeplot(year_df[topic], label=year, ax=ax[-1], legend=False)\n",
    "        \n",
    "        # Set properties for the last subplot\n",
    "        ax[-1].legend(years, fontsize='medium')\n",
    "        ax[-1].set_title(\"Density Comparison\", fontsize='medium')\n",
    "        \n",
    "        # Remove unused subplots\n",
    "        for unused_ax in ax[len(years):-1]:  # iterate over unused subplots (excluding the comparison plot)\n",
    "            fig.delaxes(unused_ax)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplots and Rolling Statisitcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_stats(df, window=30):\n",
    "\n",
    "    X = df.drop(['Date', 'Company Name', 'Bloomberg Ticker',\n",
    "                 'ISIN', 'CIQ ISIN', 'Acquired', 'Target'], axis=1)\n",
    "    \n",
    "    # Set Date from df as the index for X\n",
    "    X = X.set_index(df['Date']) \n",
    "    num_plots = X.shape[1]\n",
    "    cols = int(np.ceil(np.sqrt(num_plots)))\n",
    "    rows = int(np.ceil(num_plots / cols))\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 15))\n",
    "\n",
    "    for idx, col in enumerate(X.columns):\n",
    "        i, j = divmod(idx, cols)\n",
    "        \n",
    "        X[col].plot(ax=axes[i, j], label='Original')\n",
    "        X[col].rolling(window=window).mean().plot(ax=axes[i, j], label='Rolling Mean')\n",
    "        X[col].rolling(window=window).std().plot(ax=axes[i, j], label='Rolling Std')\n",
    "        axes[i, j].set_title(f'Rolling Stats for {col}')\n",
    "        axes[i, j].legend()\n",
    "        axes[i, j].grid(True)\n",
    "\n",
    "    # Remove extra subplots\n",
    "    for idx in range(num_plots, rows * cols):\n",
    "        i, j = divmod(idx, cols)\n",
    "        fig.delaxes(axes[i, j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_boxplot(X):\n",
    "    num_plots = X.shape[1]\n",
    "    cols = int(np.ceil(np.sqrt(num_plots)))\n",
    "    rows = int(np.ceil(num_plots / cols))\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 15))\n",
    "\n",
    "    for idx, col in enumerate(X.columns):\n",
    "        i, j = divmod(idx, cols)\n",
    "        sns.boxplot(data=X[col], ax=axes[i, j])  # Here's the change. Specify the column for data.\n",
    "        axes[i, j].set_title(f' Boxplot for {col}')\n",
    "        axes[i, j].grid(True)\n",
    "\n",
    "    # Remove extra subplots\n",
    "    for idx in range(num_plots, rows * cols):\n",
    "        i, j = divmod(idx, cols)\n",
    "        fig.delaxes(axes[i, j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming 'Date' column exists in `df` and is of `datetime64` type\n",
    "def box_plot_yearly(df):\n",
    "    df_new = df.copy()  # Create a copy to prevent modifying the original dataframe\n",
    "    df_new['year'] = df_new['Date'].dt.year\n",
    "    \n",
    "    indicators = df_new.drop(['Date', 'year', 'Company Name', 'Bloomberg Ticker',\n",
    "                              'ISIN', 'CIQ ISIN', 'Acquired', 'Target'], axis=1)\n",
    "\n",
    "    num_plots = indicators.shape[1]\n",
    "    cols = int(np.ceil(np.sqrt(num_plots)))\n",
    "    rows = int(np.ceil(num_plots / cols))\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 15))\n",
    "\n",
    "    for idx, col in enumerate(indicators.columns):\n",
    "        i, j = divmod(idx, cols)\n",
    "        sns.boxplot(x='year', y=col, data=df_new, ax=axes[i, j])\n",
    "        \n",
    "        # Shorten the x-tick labels\n",
    "        # Get unique years and shorten them\n",
    "        unique_years = df_new['year'].unique()\n",
    "        shortened_years = [str(year)[-2:] for year in unique_years]\n",
    "        axes[i, j].set_xticks(np.arange(len(unique_years)))  # Set ticks positions\n",
    "        axes[i, j].set_xticklabels(shortened_years)  # Set shortened year labels\n",
    "        \n",
    "        axes[i, j].set_title(f'Yearly Boxplot for {col}')\n",
    "        axes[i, j].grid(True)\n",
    "\n",
    "    # Remove extra subplots\n",
    "    for idx in range(num_plots, rows * cols):\n",
    "        i, j = divmod(idx, cols)\n",
    "        fig.delaxes(axes[i, j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_IQR(df, threshold):\n",
    "    # Store columns with outliers for later printing\n",
    "    outliers_dict = {}\n",
    "    \n",
    "    for column in df.columns:\n",
    "        # Exclude NaN values for the current column\n",
    "        column_data = df[column].dropna()\n",
    "        \n",
    "        # Calculate Q1 and Q3\n",
    "        Q1 = column_data.quantile(0.25)\n",
    "        Q3 = column_data.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # Define bounds for the outliers\n",
    "        lower_bound = Q1 - threshold * IQR ## 1.5 or 3.0\n",
    "        upper_bound = Q3 + threshold * IQR\n",
    "        \n",
    "        # Find outliers (excluding NaN values)\n",
    "        outliers = df.loc[column_data.index][(column_data < lower_bound) | (column_data > upper_bound)]\n",
    "        \n",
    "        # If there are outliers, print and store them\n",
    "        if not outliers.empty:\n",
    "            #print(f\"Outliers for {column}:\")\n",
    "            #print(outliers)\n",
    "            outliers_dict[column] = outliers\n",
    "        \n",
    "        # Remove the outliers from the dataframe\n",
    "        df = df.drop(outliers.index)\n",
    "    \n",
    "    if not outliers_dict:\n",
    "        print(\"No outliers found.\")\n",
    "    return df\n",
    "\n",
    "def remove_outliers_with_zscore(df, threshold=3):\n",
    "    # Store columns with outliers for later printing\n",
    "    outliers_dict = {}\n",
    "    \n",
    "    for column in df.columns:\n",
    "        # Skip columns with non-numeric data types\n",
    "        if pd.api.types.is_numeric_dtype(df[column]):\n",
    "            # Calculate z-scores\n",
    "            zs = zscore(df[column].dropna())\n",
    "            \n",
    "            # Identify the outliers\n",
    "            outliers = df[column].dropna()[(abs(zs) > threshold)]\n",
    "            \n",
    "            # If there are outliers, print and store them\n",
    "            if not outliers.empty:\n",
    "                print(f\"Outliers for {column}:\")\n",
    "                print(outliers)\n",
    "                outliers_dict[column] = outliers\n",
    "                \n",
    "                # Remove the rows with outliers\n",
    "                df = df.drop(outliers.index)\n",
    "    \n",
    "    if not outliers_dict:\n",
    "        print(\"No outliers found.\")\n",
    "        \n",
    "    return df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation(X):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    corr = pd.DataFrame(X).corr()\n",
    "    heatmap = sns.heatmap(corr, vmin=-1, vmax=1, center=0, annot=True, cmap=\"RdBu_r\")\n",
    "    heatmap.set_title(\"Correlation Heatmap\", fontdict={\"fontsize\":16})\n",
    "    plt.show()\n",
    "\n",
    "def plot_correlation_triangle(X):\n",
    "    # Modify the mask to hide only the strict upper triangle\n",
    "    mask = np.triu(np.ones_like(X.corr(), dtype=bool), k=1)\n",
    "    \n",
    "    plt.figure(figsize=(16, 6))\n",
    "    heatmap = sns.heatmap(X.corr(), mask=mask, vmin=-1, vmax=1, annot=True, center=0, cmap='RdBu_r')\n",
    "    heatmap.set_title('Triangle Correlation Heatmap')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance with Univariate Statistical Tests\n",
    "This method involves evaluating the statistical relationship between each feature and the target variable independently. Common statistical tests such as chi-square, ANOVA, or correlation coefficients can be used to quantify the strength of the relationship. Features with higher test statistics or p-values below a certain threshold are considered more important. For instance, here we use chi-square test for categorical features and ANOVA for numerical features.\n",
    "\n",
    "https://medium.com/datadriveninvestor/five-approaches-to-determine-feature-importance-in-classification-problems-7be5daef37a6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kruskal\n",
    "from scipy.stats import f_oneway, yeojohnson\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def categorical_feature_importance(X, y):\n",
    "    categorical_features = X.select_dtypes(include=['category', 'object']).columns\n",
    "\n",
    "    chi2_scores, p_values = [], []\n",
    "    \n",
    "    for feature in categorical_features:\n",
    "        contingency_table = pd.crosstab(X[feature], y)\n",
    "        chi2, p_value, _, _ = chi2_contingency(contingency_table)\n",
    "        chi2_scores.append(chi2)\n",
    "        p_values.append(p_value)\n",
    "\n",
    "    results = pd.DataFrame({'Feature': categorical_features, 'Chi2 Score': chi2_scores, 'P-value': p_values})\n",
    "    cat_res = results.sort_values(by='P-value')\n",
    "\n",
    "    print(\"Categorical Features:\")\n",
    "    print(cat_res)\n",
    "\n",
    "def one_way_anova_with_yeo_johnson_trafo(X, y):\n",
    "    numerical_features = X.select_dtypes(include=[float, int]).columns\n",
    "    f_scores, p_values = [], []\n",
    "\n",
    "    for feature in numerical_features:\n",
    "        groups = [X[y == c][feature].dropna() for c in np.unique(y)]\n",
    "        transformed_groups = [yeojohnson(group)[0] for group in groups]\n",
    "        f, p_value = f_oneway(*transformed_groups)\n",
    "        f_scores.append(f)\n",
    "        p_values.append(p_value)\n",
    "\n",
    "    results = pd.DataFrame({'Feature': numerical_features, 'F-score': f_scores, 'P-value': p_values})\n",
    "    num_res = results.sort_values(by='P-value')\n",
    "\n",
    "    print(\"\\nNumerical Features:\")\n",
    "    print(num_res)\n",
    "\n",
    "def numerical_feature_importance_kruskal(X, y):\n",
    "    numerical_features = X.select_dtypes(include=[float, int]).columns\n",
    "    h_stats, p_values = [], []\n",
    "\n",
    "    for feature in numerical_features:\n",
    "        groups = [X[y == c][feature].dropna() for c in np.unique(y)]\n",
    "        H, p_value = kruskal(*groups)\n",
    "        h_stats.append(H)\n",
    "        p_values.append(p_value)\n",
    "\n",
    "    results = pd.DataFrame({'Feature': numerical_features, 'H-statistic': h_stats, 'P-value': p_values})\n",
    "    num_res = results.sort_values(by='P-value')\n",
    "\n",
    "    print(\"\\nNumerical Features:\")\n",
    "    print(num_res)\n",
    "\n",
    "\n",
    "def feature_importance_stats_test(X_train, y_train):\n",
    "    #categorical_features = X_train.select_dtypes(include='category').columns\n",
    "    #numerical_features = X_train.select_dtypes(include=float).columns\n",
    "    categorical_feature_importance(X_train, y_train)\n",
    "    print(\"---------------------------------------------\")\n",
    "    one_way_anova_with_yeo_johnson_trafo(X_train, y_train)\n",
    "    print(\"---------------------------------------------\")\n",
    "    numerical_feature_importance_kruskal(X_train, y_train)\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Initialization\n",
    "\n",
    "Loading of Data and Initalization of the dataset.\n",
    "Preform EDA, show plots and distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing dataset path\n",
    "path = os.path.join(sys.path[0])\n",
    "# Loading and Handling dataset\n",
    "mydata = OurDataset(path)\n",
    "\n",
    "### Removing and sorting stuff\n",
    "mydata.remove_timestamps()\n",
    "mydata.remove_duplicates()\n",
    "mydata.sort_after_date()\n",
    "mydata.clean_multiproducts()\n",
    "mydata.remove_stocks_with_lacking_data(min_datapoints=2)\n",
    "\n",
    "\n",
    "# Drop 'Financial_Ratio_11', and 'Financial_Ratio_14' otherwise dataset is too small and has too much noise!\n",
    "mydata.remove_columns(columns = ['Financial_Ratio_11', 'Financial_Ratio_14']) \n",
    "mydata.data.to_excel(\"Cleaned_Data.xlsx\")\n",
    "# Splitting the data into train, validation and test sets! Also does missing values imputation, outlier cleaning and shuffle or not shuffle!\n",
    "mydata.split_dataset(imputer = \"iterative\", remove_outliers=True, shuffle=True)\n",
    "\n",
    "# Renaming\n",
    "X_train, X_test, X_val, y_train, y_test, y_val = mydata.X_train, mydata.X_test, mydata.X_val, mydata.y_train, mydata.y_test, mydata.y_val\n",
    "\n",
    "###Check Format\n",
    "print(\"----------------------------------\")\n",
    "print(\"----dTypes of X_train----------\")\n",
    "print(X_train.dtypes)\n",
    "print(\"----dTypes of X_val----------\")\n",
    "print(X_val.dtypes)\n",
    "print(\"----dTypes of X_test----------\")\n",
    "print(X_test.dtypes)\n",
    "print(\"----------------------------------\")\n",
    "print(\"dtype y_train\", y_train.dtype)\n",
    "print(\"dtype y_val\", y_val.dtype)\n",
    "print(\"dtype y_test\", y_test.dtype)\n",
    "print(\"----------------------------------\")\n",
    "print(\"has missing values?\", X_train.isnull().values.any())\n",
    "print(\"has missing values?\", X_val.isnull().values.any())\n",
    "print(\"has missing values?\", X_test.isnull().values.any())\n",
    "print(\"----------------------------------\")\n",
    "print(\"Class Disitbution for y_train\")\n",
    "plot_traget_distribution_simple(y_train)\n",
    "print(\"Class Disitbution for y_val\")\n",
    "plot_traget_distribution_simple(y_val)\n",
    "print(\"Class Disitbution for y_test\")\n",
    "plot_traget_distribution_simple(y_test)\n",
    "print(\"----------------------------------\")\n",
    "print(\"----------------------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Printing Plots/EDA/Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting and Printing all the EDA Stuff\n",
    "def do_eda( df, X, y):\n",
    "    #MissingData Stuff\n",
    "    plot_missing_data_tabel(X)\n",
    "    plot_missing_data_simple(X)\n",
    "    plot_missing_data_heatmap(df)\n",
    "    plot_missing_data_pca(X, y)\n",
    "    plot_missing_data_pca_3d(X, y)\n",
    "    plot_missing_data_pca_3d_only_arrows(X, y)\n",
    "    #TargetDistribution Stuff\n",
    "    plot_traget_distribution_simple(y)\n",
    "    plot_target_distribution_yearly(df)\n",
    "    #BoxPlot Stuff\n",
    "    plot_boxplot(X)\n",
    "    box_plot_yearly(df)\n",
    "    #Histogramm Stuff\n",
    "    rolling_stats(df)\n",
    "    plot_mean_median_hist(X, y) \n",
    "    plot_hist_yearly(df)\n",
    "    plot_histogram_scaled_for_KDE(X)\n",
    "    plot_histogram(X)\n",
    "    #Correlation Matrix\n",
    "    plot_correlation_triangle(X) \n",
    "  \n",
    "\n",
    "def do_eda_validation(X_clean, y_clean):\n",
    "    numerical_features = X_clean.select_dtypes(include='float64')\n",
    "    plot_traget_distribution_simple(y_clean)\n",
    "    #plot_histogram_scaled_for_KDE(numerical_features)\n",
    "    #plot_histogram(numerical_features)\n",
    "    plot_mean_median_hist(numerical_features, y_clean)\n",
    "    #plot_boxplot(numerical_features)\n",
    "    plot_correlation_triangle(numerical_features) \n",
    "    feature_importance_stats_test(X_clean, y_clean)\n",
    "\n",
    "# Function to compare central tendency and spread\n",
    "def compare_data_not_scaled(df1, df2):\n",
    "    stats = ['mean', 'median', 'std', 'var']\n",
    "    comparison_stats = pd.DataFrame(index=df1.columns, columns=pd.MultiIndex.from_product([stats, ['Not Imputed', 'Imputed']]))\n",
    "    \n",
    "    for col in df1.columns:\n",
    "        for stat in stats:\n",
    "            comparison_stats.loc[col, (stat, 'Not Imputed')] = getattr(df1[col], stat)()\n",
    "            comparison_stats.loc[col, (stat, 'Imputed')] = getattr(df2[col], stat)()\n",
    "    \n",
    "    print(comparison_stats)\n",
    "    # Plotting the comparison results\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 8))\n",
    "    fig.suptitle('Comparison of Central Tendency and Spread Before and After Imputation', y=1.03)\n",
    "\n",
    "    for ax, stat in zip(axes.flatten(), ['mean', 'median', 'std', 'var']):\n",
    "        comparison_stats[stat].plot(kind='bar', ax=ax)\n",
    "        ax.set_title(stat.capitalize())\n",
    "        ax.set_ylabel(stat.capitalize())\n",
    "        ax.legend(title='Dataset')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def compare_data(df1, df2):\n",
    "    stats = ['mean', 'median', 'std', 'var']\n",
    "    comparison_stats = pd.DataFrame(index=df1.columns, columns=pd.MultiIndex.from_product([stats, ['Not Imputed', 'Imputed']]))\n",
    "    \n",
    "    for col in df1.columns:\n",
    "        for stat in stats:\n",
    "            if stat in ['std', 'var']:\n",
    "                comparison_stats.loc[col, (stat, 'Not Imputed')] = np.log1p(getattr(df1[col], stat)())\n",
    "                comparison_stats.loc[col, (stat, 'Imputed')] = np.log1p(getattr(df2[col], stat)())\n",
    "            else:\n",
    "                comparison_stats.loc[col, (stat, 'Not Imputed')] = getattr(df1[col], stat)()\n",
    "                comparison_stats.loc[col, (stat, 'Imputed')] = getattr(df2[col], stat)()\n",
    "    \n",
    "    print(comparison_stats)\n",
    "    # Plotting the comparison results\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 8))\n",
    "    fig.suptitle('Comparison of Central Tendency and Spread Before and After Imputation', y=1.03)\n",
    "\n",
    "    for ax, stat in zip(axes.flatten(), stats):\n",
    "        if stat in ['std', 'var']:\n",
    "            ax.set_yscale('log')\n",
    "            ax.set_ylabel(f'Log {stat.capitalize()}')\n",
    "        else:\n",
    "            ax.set_ylabel(stat.capitalize())\n",
    "            \n",
    "        comparison_stats[stat].plot(kind='bar', ax=ax)\n",
    "        ax.set_title(stat.capitalize())\n",
    "        ax.legend(title='Dataset')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def qqplot_columns(df_not_imputed, df_imputed, cols):\n",
    "    \"\"\"\n",
    "    Creates QQ plots and box plots for the specified columns in the non-imputed and imputed DataFrames.\n",
    "\n",
    "    Parameters:\n",
    "    df_not_imputed (pd.DataFrame): DataFrame before imputation\n",
    "    df_imputed (pd.DataFrame): DataFrame after imputation\n",
    "    cols (list of str): List of column names to plot\n",
    "    \"\"\"\n",
    "    for col in cols:\n",
    "        fig = plt.figure(figsize=(12, 5))\n",
    "        gs = gridspec.GridSpec(1, 2, width_ratios=[3, 1]) \n",
    "        ax0 = plt.subplot(gs[0])\n",
    "        \n",
    "        not_imputed_sorted = np.sort(df_not_imputed[col].dropna())\n",
    "        imputed_sorted = np.sort(df_imputed[col])\n",
    "        \n",
    "        # Compute the theoretical quantiles for not imputed data\n",
    "        not_imputed_quantiles = stats.probplot(not_imputed_sorted, dist=\"norm\")[0][0]\n",
    "        # Compute the theoretical quantiles for imputed data\n",
    "        imputed_quantiles = stats.probplot(imputed_sorted, dist=\"norm\")[0][0]\n",
    "        \n",
    "        ax0.plot(not_imputed_quantiles, not_imputed_sorted, '.', color='blue', label='Not Imputed')\n",
    "        ax0.plot(not_imputed_quantiles, np.poly1d(np.polyfit(not_imputed_quantiles, not_imputed_sorted, 1))(not_imputed_quantiles), color='blue', linestyle='--')\n",
    "        \n",
    "        ax0.plot(imputed_quantiles, imputed_sorted, '.', color='orange', label='Imputed')\n",
    "        ax0.plot(imputed_quantiles, np.poly1d(np.polyfit(imputed_quantiles, imputed_sorted, 1))(imputed_quantiles), color='orange', linestyle='--')\n",
    "        \n",
    "        ax0.set_title(f'QQ Plot for {col}')\n",
    "        ax0.set_xlabel('Theoretical Quantiles')\n",
    "        ax0.set_ylabel(f'Sorted Values of {col}')\n",
    "        ax0.legend()\n",
    "        \n",
    "        ax1 = plt.subplot(gs[1])\n",
    "        sns.boxplot(data=pd.concat([df_not_imputed[col], df_imputed[col]], axis=1, keys=['Not Imputed', 'Imputed']), ax=ax1)\n",
    "        ax1.set_title('Boxplot Comparison')\n",
    "        ax1.set_ylabel(f'Values of {col}')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##############################################\n",
    "\n",
    "\n",
    "#do_eda(mydata.data.copy(deep=True), mydata.X.copy(deep=True), mydata.y.copy(deep=True))\n",
    "X_combined = pd.concat([X_train, X_val], axis=0, ignore_index=True)\n",
    "y_combined =  pd.concat([y_train, y_val], axis =0, ignore_index=True)\n",
    "X_combined.to_excel(\"X_combined.xlsx\")\n",
    "#do_eda_validation(X_combined, y_combined)\n",
    "\n",
    "# Running the comparison\n",
    "X_not_imputed_combined = pd.concat([mydata.X_train_not_imputed, mydata.X_val_not_imputed], axis=0, ignore_index=True)\n",
    "#compare_data(X_not_imputed_combined, X_combined.select_dtypes(include='float64'))\n",
    "# Creating QQ-plots for all columns\n",
    "qqplot_columns(X_not_imputed_combined, X_combined.select_dtypes(include='float64'), X_not_imputed_combined.columns)\n",
    "#####\n",
    "#X_combined_all = pd.concat([X_combined, X_test], axis=0, ignore_index=True)\n",
    "#test_stationarity(X_combined_all.select_dtypes(include='float64'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Parameters:\n",
    "## Sampling\n",
    "\n",
    "We use the following sampling methods:\n",
    "\n",
    "1. RandomOverSampler: category and continuous\n",
    "2. SMOTE (Synthetic Minority Over-sampling Technique) only for continuous data\n",
    "3. BorderlineSMOTE only for continuous data\n",
    "4. SMOTENC for categorical data!!!!\n",
    "\n",
    "\n",
    "### Scaling\n",
    "\n",
    "Scaling is a necessary pre-processing step for certain machine learning algorithms, especially those that rely on the calculation of distances or optimization methods.\n",
    "\n",
    "Here are some examples of algorithms where scaling is crucial:\n",
    "- Linear and Logistic Regression\n",
    "- Support Vector Machines (SVM)\n",
    "- K-Nearest Neighbors (K-NN)\n",
    "- Neural Networks\n",
    "- Principal Component Analysis (PCA)\n",
    "\n",
    "However, there are also algorithms where scaling isn't necessary:\n",
    "- Tree-based algorithms: Algorithms like Decision Trees, Random Forests, Gradient Boosting, and XGBoost do not require feature scaling. These algorithms are not distance-based and can handle various scales.\n",
    "- Naive Bayes: Naive Bayes is not affected by the feature scales as it is not distance based."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unified parameters for pipeline's random search\n",
    "scaler = StandardScaler()\n",
    "mms = MinMaxScaler()\n",
    "ros = RandomOverSampler(random_state = 42) # for continuous and category \n",
    "smote = SMOTE(random_state= 42) # only for continuous data\n",
    "borderline_smote = BorderlineSMOTE(random_state=42) # only for continuous data\n",
    "smote_nc = SMOTENC(categorical_features=\"auto\", random_state=42) ######## Important for Categorical Data!!!\n",
    "kFold = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 42) # n_splits=5\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainModel:\n",
    "    def __init__(self, X_train, y_train, X_test, y_test, X_val, y_val, model=None, pipeline=None, model_params=None, pipeline_params=None, verbose=0):\n",
    "        # Data Attributes\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "\n",
    "        # Model and Pipeline\n",
    "        self.model = model\n",
    "        self.pipe = pipeline\n",
    "        self.model_params = model_params\n",
    "        self.pipeline_params = pipeline_params\n",
    "\n",
    "        # Other Attributes\n",
    "        self.verbose = verbose\n",
    "        self.result = None  # will hold the trained model or pipeline\n",
    "\n",
    "        # Cross-validation setup\n",
    "        number_of_splits = min(10, len(X_train)) # or 5 K-Fold?\n",
    "        self.kFold = StratifiedKFold(n_splits=number_of_splits, shuffle=True, random_state=42) # True or False beim Shuffle?!\n",
    "\n",
    "    def train_with_crossvalidation(self, use_pipeline=True, use_grid_search=True): \n",
    "        # Error checking\n",
    "        if use_pipeline and (not self.pipe or not self.pipeline_params):\n",
    "            raise ValueError(\"Pipeline or pipeline parameters not provided!\")\n",
    "        if not use_pipeline and (not self.model or not self.model_params):\n",
    "            raise ValueError(\"Model or model parameters not provided!\")\n",
    "        \n",
    "        X_combined = pd.concat([self.X_train, self.X_val], axis=0, ignore_index=True).reset_index(drop=True) \n",
    "        y_combined =  pd.concat([self.y_train, self.y_val], axis=0, ignore_index=True).reset_index(drop=True) \n",
    "        \n",
    "\n",
    "        # Select appropriate searcher\n",
    "        if use_pipeline:\n",
    "            estimator = self.pipe\n",
    "            params = self.pipeline_params\n",
    "        else:\n",
    "            estimator = self.model\n",
    "            params = self.model_params\n",
    "\n",
    "        if use_grid_search:\n",
    "            searcher = GridSearchCV(estimator=estimator, \n",
    "                                    param_grid=params, \n",
    "                                    scoring=\"f1_weighted\", \n",
    "                                    cv=self.kFold, \n",
    "                                    n_jobs=-1, \n",
    "                                    verbose=self.verbose)\n",
    "        else:\n",
    "            searcher = RandomizedSearchCV(estimator=estimator, \n",
    "                                          param_distributions=params, \n",
    "                                          scoring=\"f1_weighted\", \n",
    "                                          cv=self.kFold, \n",
    "                                          n_iter=30,\n",
    "                                          n_jobs=-1,\n",
    "                                          random_state=42, \n",
    "                                          verbose=self.verbose)\n",
    "            \n",
    "        searcher.fit(X_combined, y_combined)\n",
    "        self.result = searcher.best_estimator_ #more efficient than just saving searcher\n",
    "\n",
    "        # Informative print statement\n",
    "        print(f\"Best {'pipeline' if use_pipeline else 'model'} params used:\", searcher.best_params_)\n",
    "\n",
    "    def train_with_validation(self, use_pipeline=True, use_grid_search=True):\n",
    "        \"\"\"\n",
    "        Train the model using a separate validation set for model selection.\n",
    "        \"\"\"\n",
    "        # Error checking\n",
    "        if use_pipeline and (not self.pipe or not self.pipeline_params):\n",
    "            raise ValueError(\"Pipeline or pipeline parameters not provided!\")\n",
    "        if not use_pipeline and (not self.model or not self.model_params):\n",
    "            raise ValueError(\"Model or model parameters not provided!\")\n",
    "        \n",
    "        # Select appropriate model/estimator\n",
    "        if use_pipeline:\n",
    "            estimator = clone(self.pipe)\n",
    "            params = self.pipeline_params\n",
    "        else:\n",
    "            estimator = clone(self.model)\n",
    "            params = self.model_params\n",
    "        \n",
    "        # 1. Train the Model on the Training Set\n",
    "        #estimator.fit(self.X_train, self.y_train) #you don't need to manually fit the model on the training set before this process as the GridSearchCV\n",
    "        \n",
    "        # 2. Hyperparameter Tuning on the Validation Set\n",
    "        test_fold = np.hstack([-np.ones(len(self.X_train)), np.zeros(len(self.X_val))])\n",
    "        ps = PredefinedSplit(test_fold)\n",
    "        \n",
    "        if use_grid_search:\n",
    "            searcher = GridSearchCV(estimator, param_grid=params, scoring=\"f1_weighted\", cv=ps, n_jobs=-1, verbose=self.verbose)\n",
    "        else:\n",
    "            searcher = RandomizedSearchCV(estimator, param_distributions=params, scoring=\"f1_weighted\", n_iter=30, n_jobs=-1, random_state=42, cv=ps, verbose=self.verbose)\n",
    "        \n",
    "        # Note: You need to combine the training and validation set for fitting\n",
    "        X_combined = pd.concat([self.X_train, self.X_val], axis=0 , ignore_index=True)\n",
    "        y_combined =  pd.concat([self.y_train, self.y_val], axis=0, ignore_index=True)\n",
    "        searcher.fit(X_combined, y_combined) \n",
    "        \n",
    "        # Setting the best estimator as the final model\n",
    "        self.result = searcher.best_estimator_\n",
    "        \n",
    "        # Informative print statement\n",
    "        print(f\"Best {'pipeline' if use_pipeline else 'model'} params used:\", searcher.best_params_)\n",
    "   \n",
    "    \n",
    "    def evaluate_model(self, plot=True):\n",
    "        \"\"\"\n",
    "        Evaluate the best trained model or pipeline.\n",
    "        \"\"\"\n",
    "        if self.result is None:\n",
    "            raise ValueError(\"Model has not been trained yet!\")\n",
    "        \n",
    "        y_pred = self.result.predict(self.X_test)\n",
    "        self.report(y_pred, plot)\n",
    "\n",
    "    def report(self, y_pred, plot=True):\n",
    "        \"\"\"\n",
    "        Print classification report and display confusion matrix.\n",
    "        \"\"\"\n",
    "        class_report = classification_report(self.y_test, y_pred)\n",
    "        print(class_report)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(self.y_test, y_pred) # normalize=\"true\" gives the probortions!!!!\n",
    "        print(conf_matrix)\n",
    "\n",
    "        if plot:\n",
    "            # Infer unique labels from y_test, y_test has to be dataframe\n",
    "            labels = sorted(list(set(self.y_test)))\n",
    "            label_strings = [str(label) for label in labels]\n",
    "\n",
    "            df_cm = pd.DataFrame(conf_matrix, label_strings, label_strings)\n",
    "            # Naming the conf_matrix df_cm=pd.DataFrame(conf_matrix, [\"Sell 0\", \"Buy 1\"],  [\"Sell 0\", \"Buy 1\"])\n",
    "            sns.heatmap(df_cm, fmt='d',annot=True).set(xlabel=\"Assigned Class\", ylabel=\"True Class\", title=\"Confusion Matrix\")\n",
    "            plt.show()\n",
    "            ## Add plot_curves_roc directly\n",
    "            self.plot_curves_roc()\n",
    "            \n",
    "    def plot_curves_roc(self):\n",
    "        \"\"\"\n",
    "        Plot ROC and Precision-Recall curves.\n",
    "        \"\"\"\n",
    "        y_pred_prob = self.result.predict_proba(self.X_test)\n",
    "        \n",
    "        # Setting up subplots\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "        # ROC Curve and AUC score\n",
    "        fpr, tpr, thresholds = roc_curve(self.y_test, y_pred_prob[:, 1]) #Only works for binary classification here\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        ax1.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "        ax1.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        ax1.set_xlabel('False Positive Rate')\n",
    "        ax1.set_ylabel('True Positive Rate')\n",
    "        ax1.set_title('Receiver Operating Characteristic (ROC) Curve')\n",
    "        ax1.legend()\n",
    "\n",
    "        # Precision-Recall Curve\n",
    "        precision, recall, _ = precision_recall_curve(self.y_test, y_pred_prob[:, 1])\n",
    "        avg_precision = average_precision_score(self.y_test, y_pred_prob[:, 1])\n",
    "        ax2.plot(recall, precision, color='darkorange', lw=2, label='Precision-Recall curve (area = %0.2f)' % avg_precision)\n",
    "        ax2.set_xlabel('Recall')\n",
    "        ax2.set_ylabel('Precision')\n",
    "        ax2.set_title('Precision-Recall Curve')\n",
    "        ax2.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_learning_curve(self, step_size=0.1):\n",
    "        \"\"\"\n",
    "        Plot the learning curve for the model.\n",
    "        step_size: Fraction of the maximum size of the training set that will be used in iterations.\n",
    "                   e.g., step_size=0.1 will use 10%, 20%, ..., 100% of the training data.\n",
    "        \"\"\"\n",
    "        train_sizes, train_scores, test_scores = learning_curve(\n",
    "            estimator=self.result, \n",
    "            X=self.X_train, \n",
    "            y=self.y_train, \n",
    "            cv=self.kFold,\n",
    "            train_sizes=np.linspace(0.1, 1.0, int(1/step_size)), \n",
    "            scoring=\"f1_weighted\",\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        train_scores_mean = np.mean(train_scores, axis=1)\n",
    "        train_scores_std = np.std(train_scores, axis=1)\n",
    "        test_scores_mean = np.mean(test_scores, axis=1)\n",
    "        test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "        plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "        plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "        plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "        plt.xlabel(\"Training examples\")\n",
    "        plt.ylabel(\"F1 Score\")\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.grid(True)\n",
    "        plt.title(\"Learning Curve\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def selected_features(self):\n",
    "            \"\"\"\n",
    "            Display the features selected by the feature selection step in the pipeline. Ensure that in the pipeline this step is named 'feature_selection'.\n",
    "            \"\"\"\n",
    "            # Get the feature selector from the pipeline\n",
    "            feature_selector = self.result.named_steps[\"feature_selection\"] #self.result.best_estimator_.named_steps[\"feature_selection\"]\n",
    "            # Get the mask of selected features\n",
    "            selected_mask = feature_selector.get_support()\n",
    "            # Get the feature names from the original dataset\n",
    "            feature_names = self.X_train.columns\n",
    "            # Print the selected feature names\n",
    "            selected_features = feature_names[selected_mask]\n",
    "            print(\"Selected features:\", selected_features)\n",
    "\n",
    "    def pca_components(self):\n",
    "        \"\"\"\n",
    "        Display the principal components and their relationship to the original features \n",
    "        if using PCA, or the eigenvalues if using KernelPCA. Ensure that in the pipeline this step is named 'feature_selection'.\n",
    "        \"\"\"\n",
    "        # Get the PCA or KernelPCA transformer from the pipeline\n",
    "        transformer = self.result.named_steps[\"feature_selection\"] #self.result.best_estimator_.named_steps[\"feature_selection\"]\n",
    "        if isinstance(transformer, PCA):\n",
    "            # For PCA\n",
    "            # Get the components\n",
    "            components = transformer.components_\n",
    "            # Get the feature names from the original dataset\n",
    "            feature_names = self.X_train.columns\n",
    "            # Print the components and their relationship to the original features\n",
    "            for i, component in enumerate(components):\n",
    "                print(f\"Principal Component {i+1}:\")\n",
    "                for feature, weight in zip(feature_names, component):\n",
    "                    print(f\"{feature}: {weight:.4f}\")\n",
    "                print(\"\\n\")\n",
    "\n",
    "        elif isinstance(transformer, KernelPCA):\n",
    "            # For KernelPCA\n",
    "            # Get the eigenvalues\n",
    "            eigenvalues = transformer.eigenvalues_\n",
    "\n",
    "            # Print the eigenvalues\n",
    "            for i, value in enumerate(eigenvalues):\n",
    "                print(f\"Eigenvalue for Kernel Principal Component {i+1}: {value:.4f}\")\n",
    "            print(\"\\n\")\n",
    "        else:\n",
    "            print(\"The feature_selection step is neither PCA nor KernelPCA.\")\n",
    "\n",
    "    def feature_importance_plot_shap(self, number_of_sample=100):\n",
    "        \"\"\"\n",
    "        Generate a feature importance plot using SHAP values. Ensure the model's step in the pipeline is named 'model'.\n",
    "        \"\"\"\n",
    "        # Define a wrapper to bypass potential issues\n",
    "        def model_predict(data):\n",
    "            return model.predict(data)\n",
    "\n",
    "        X_test_sample = shap.sample(self.X_test, number_of_sample)  # sampling since computationaly extensive...\n",
    "        #If you want to see how your model is making predictions on unseen/new data and which features are influencing these decisions, then you should compute the SHAP values on the test data!\n",
    "\n",
    "        # Extract model from the pipeline\n",
    "        if hasattr(self.result, 'steps'): \n",
    "            model = dict(self.result.steps).get('model', None) \n",
    "        else:\n",
    "            model = self.result \n",
    "\n",
    "        # Check model type to determine the appropriate SHAP explainer\n",
    "        if isinstance(model, (RandomForestClassifier, DecisionTreeClassifier, ExtraTreesClassifier)):\n",
    "            explainer = shap.TreeExplainer(model)\n",
    "            shap_values = explainer.shap_values(X_test_sample)\n",
    "\n",
    "        elif isinstance(model, LogisticRegression):\n",
    "            explainer = shap.LinearExplainer(model, X_test_sample, feature_dependence=\"independent\") #shap.sample(self.X_train, number_of_sample)?? Some how it does not work!!!\n",
    "            shap_values = explainer.shap_values(X_test_sample)\n",
    "        \n",
    "\n",
    "        elif isinstance(model, SVC):\n",
    "            explainer = shap.KernelExplainer(model_predict, X_test_sample) ### on X_test\n",
    "            shap_values = explainer.shap_values(X_test_sample, nsamples=\"auto\", n_jobs=-1)\n",
    "\n",
    "        elif isinstance(model, ( XGBClassifier, CatBoostClassifier)):\n",
    "            # Have to use KernelExpaliner since XGB, CatBoost are using categorical data, so TreeExpaliner wont work\n",
    "            print(\"Uses KernelExplainer for XGB and CatBoost -- this is very slow!\")\n",
    "            explainer = shap.KernelExplainer(model_predict, X_test_sample) ## on X_test\n",
    "            shap_values = explainer.shap_values(X_test_sample, nsamples=\"auto\", n_jobs=-1)\n",
    "\n",
    "        else:\n",
    "            print(\"Uses KernelExplainer which is very slow....\")\n",
    "            explainer = shap.KernelExplainer(model_predict, X_test_sample) ## on X_test\n",
    "            shap_values = explainer.shap_values(X_test_sample, nsamples=\"auto\", n_jobs=-1)\n",
    "\n",
    "        \n",
    "        # SHAP DecisionPlot, only apply this on Binary Classification Tasks!\n",
    "        #For many tree-based classifiers (like RandomForestClassifier), even in binary classification tasks,\n",
    "        #SHAP provides values for both the classes, resulting in a three-dimensional output: [number_of_classes, number_of_samples, number_of_features]. \n",
    "        if len(np.shape(shap_values)) > 2:  # Multi-class classification\n",
    "            class_index = 1  # Always consider the SHAP values for the positive class in binary classification, poitive class in our case is \"buy=1\"\n",
    "            shap_values_selected = shap_values[class_index]\n",
    "            expected_value_selected = explainer.expected_value[class_index]\n",
    "            print(\"-----Here I did something in this loop-----\")\n",
    "        else:\n",
    "            shap_values_selected = shap_values\n",
    "            expected_value_selected = explainer.expected_value\n",
    "\n",
    "        # SHAP SummaryPlot\n",
    "        shap.summary_plot(shap_values_selected, X_test_sample, feature_names=self.X_train.columns) \n",
    "\n",
    "        # SHAP DecisionPlot    \n",
    "        shap.decision_plot(expected_value_selected, shap_values_selected, feature_names=self.X_train.columns.tolist())\n",
    "\n",
    "        \n",
    "        # Different Shap Dependence Plots\n",
    "        # Get the top features based on the absolute mean SHAP value\n",
    "        top_features_indices = np.argsort(np.abs(shap_values_selected).mean(0))[-10:]\n",
    "        # Fetch the corresponding feature names\n",
    "        top_features_names = self.X_train.columns.to_numpy()[top_features_indices]\n",
    "\n",
    "        # For each of these top features, find the index in the original dataset\n",
    "        for feature in top_features_names:\n",
    "            shap.dependence_plot(feature, shap_values_selected, X_test_sample)\n",
    "\n",
    "\n",
    "        # Create SVG for a SHAP force plot for a single observation ! STILL UNDER CONSTRUCTION !!!!!\n",
    "        def shap_plot(j, explainer_Model, shap_values_Model):\n",
    "            p = shap.force_plot(explainer_Model.expected_value, shap_values_Model[j], X_test_sample.iloc[[j]], matplotlib = True, show = False)\n",
    "            plt.savefig('shap_figure.svg')\n",
    "            plt.close()\n",
    "            return(p)\n",
    "\n",
    "        #shap_plot(1, explainer, shap_values)\n",
    "\n",
    "    def permutation_importance_plot(self):\n",
    "        \"\"\"\n",
    "        Generate a feature importance plot using permutation importance.\n",
    "        \"\"\"\n",
    "        # Calculate permutation importances\n",
    "        result = permutation_importance(self.result, self.X_test, self.y_test, n_repeats=30, random_state=42, n_jobs=-1) \n",
    "        sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(10, len(self.X_train.columns) * 0.5))\n",
    "        plt.boxplot(result.importances[sorted_idx].T, vert=False, widths=1.9)\n",
    "        plt.yticks(np.arange(1, len(self.X_train.columns) + 1), np.array(self.X_train.columns)[sorted_idx])\n",
    "        plt.xlabel(\"Permutation Importance\")\n",
    "        plt.title(\"Feature Importances via Permutation\")\n",
    "        plt.show()\n",
    "\n",
    "        # Print Permutation Importance\n",
    "        feature_importances = pd.DataFrame({'Feature': self.X_train.columns, 'Importance': result.importances_mean})\n",
    "        feature_importances = feature_importances.sort_values(by='Importance', ascending=False) # Sort the features based on importance\n",
    "        print(feature_importances)\n",
    "\n",
    "    def linear_model_coefficients(self):\n",
    "        \"\"\"\n",
    "        Display coefficients for linear models. Ensure the model's step in the pipeline is named 'model'.\n",
    "        \"\"\"\n",
    "        # Check if the model is wrapped inside a pipeline\n",
    "        if hasattr(self.result, 'steps'): \n",
    "            model = dict(self.result.steps).get('model', None) \n",
    "        else:\n",
    "            model = self.result \n",
    "\n",
    "        if isinstance(model, (LogisticRegression, LinearRegression)):\n",
    "            coeffs = model.coef_\n",
    "            feature_names = self.X_train.columns\n",
    "            print(\"linear model coefficients:\")\n",
    "            # Display the coefficients\n",
    "            for feature, coeff in zip(feature_names, coeffs.ravel()):\n",
    "                print(f\"{feature}: {coeff:.4f}\")\n",
    "        else:\n",
    "            print(\"The model is not a linear model.\")\n",
    "\n",
    "\n",
    "    def tree_based_importance(self):\n",
    "        \"\"\"\n",
    "        Display feature importances for tree-based models. Ensure the model's step in the pipeline is named 'model'.\n",
    "        \"\"\"\n",
    "        # Check if the model is wrapped inside a pipeline\n",
    "        if hasattr(self.result, 'steps'): \n",
    "            model = dict(self.result.steps).get('model', None) \n",
    "        else:\n",
    "            model = self.result \n",
    "\n",
    "        # Check if the model has the 'feature_importances_' attribute\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importances = model.feature_importances_\n",
    "            feature_names = self.X_train.columns\n",
    "            sorted_idx = np.argsort(importances)[::-1]\n",
    "            \n",
    "            plt.figure(figsize=(10, len(self.X_train.columns) * 0.5))\n",
    "            # Plot the features in reverse order for correct display\n",
    "            plt.barh(np.array(feature_names)[sorted_idx][::-1], importances[sorted_idx][::-1])\n",
    "\n",
    "            # Adding feature importance values next to the bars\n",
    "            for idx, value in enumerate(importances[sorted_idx][::-1]):\n",
    "                plt.text(value, idx, f\"{value:.4f}\", va='center')\n",
    "\n",
    "            plt.xlabel(\"Feature Importance\")\n",
    "            plt.title(\"Feature Importances from Tree-based Model\")\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"The model doesn't have a feature_importances_ attribute.\")\n",
    "\n",
    "\n",
    "    def correlation_importance(self):\n",
    "        \"\"\"\n",
    "        Display feature importance based on correlation with the target.\n",
    "        \"\"\"\n",
    "        # Compute correlations of features with the target\n",
    "        X_to_use=self.X_train.select_dtypes(include='category')\n",
    "        correlations = X_to_use.join(self.y_train).corr()[self.y_train.name]\n",
    "        \n",
    "        # Drop the target itself from the correlations\n",
    "        correlations = correlations.drop(self.y_train.name, errors='ignore')\n",
    "        \n",
    "        # Sort indices by absolute correlation value in descending order\n",
    "        sorted_idx = correlations.abs().sort_values(ascending=False).index\n",
    "        \n",
    "        plt.figure(figsize=(10, len(X_to_use.columns) * 0.5))\n",
    "        # Plot the features in reverse order for correct display\n",
    "        plt.barh(sorted_idx[::-1], correlations[sorted_idx][::-1])\n",
    "\n",
    "        # Adding correlation values next to the bars\n",
    "        for idx, value in enumerate(correlations[sorted_idx][::-1]):\n",
    "            plt.text(value, idx, f\"{value:.4f}\", va='center')\n",
    "\n",
    "        plt.xlabel(\"Correlation with Target\")\n",
    "        plt.title(\"Feature Importances based on Correlation\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def analyse_feature_importance_all(self, number_of_sample=100):\n",
    "        self.feature_importance_plot_shap(number_of_sample)\n",
    "        self.permutation_importance_plot()\n",
    "        self.linear_model_coefficients()\n",
    "        self.tree_based_importance()\n",
    "        self.correlation_importance()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics\n",
    "\n",
    "The F1 score is a measure of a model's accuracy that considers both precision and recall. It's the harmonic mean of these two metrics. Precision refers to the percentage of your results which are relevant, while recall refers to the percentage of total relevant results correctly classified by your algorithm.\n",
    "\n",
    "The \"weighted\" F1 score means that each class's F1 score is weighted by the number of true instances for that class. This is useful in multi-class classification problems where you have an imbalanced dataset. That is, the number of instances of each class varies greatly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_cv(func, X_test, y_test):\n",
    "  std_best_score = func.cv_results_[\"std_test_score\"][func.best_index_]\n",
    "  print(f\"Best parameters: {func.best_params_}\")\n",
    "  print(f\"Mean CV score: {func.best_score_: .6f}\")\n",
    "  print(f\"Standard deviation of CV score: {std_best_score: .6f}\")\n",
    "  print(\"Test Score: {:.6f}\".format(func.score(X_test, y_test)))\n",
    "\n",
    "def final_report(y_true, y_pred):\n",
    "  class_report = metrics.classification_report(y_true, y_pred)\n",
    "  print(class_report)\n",
    "  cm = confusion_matrix(y_true, y_pred) ## normalize = \"true\" gives the proportions!!!!!\n",
    "  cm = pd.DataFrame(cm, [\"Sell\",  \"Buy\"], [\"Sell\", \"Buy\"])\n",
    "  plt.figure(figsize = (10,5))\n",
    "  sns.heatmap(cm, annot = True, fmt='d', cmap = \"Blues\").set(xlabel = \"Assigned Class\", ylabel = \"True Class\", title = \"Confusion Matrix\") #fmt = \".2%\", \n",
    "  plt.show()\n",
    "  \n",
    "def plot_curves_roc(model, X_test, y_test):\n",
    "    # Predicting probabilities\n",
    "    y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "    # Setting up subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # ROC Curve and AUC score\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    ax1.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    ax1.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    ax1.set_xlabel('False Positive Rate')\n",
    "    ax1.set_ylabel('True Positive Rate')\n",
    "    ax1.set_title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    ax1.legend()\n",
    "\n",
    "    # Precision-Recall Curve\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)\n",
    "    avg_precision = average_precision_score(y_test, y_pred_prob)\n",
    "    ax2.plot(recall, precision, color='darkorange', lw=2, label='Precision-Recall curve (area = %0.2f)' % avg_precision)\n",
    "    ax2.set_xlabel('Recall')\n",
    "    ax2.set_ylabel('Precision')\n",
    "    ax2.set_title('Precision-Recall Curve')\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "We implemented the following different feature selections functions:\n",
    "\n",
    "- Random Forest\n",
    "- Xg Boost\n",
    "- PCA\n",
    "\n",
    "### Why Feature Selection?\n",
    "\n",
    "Feature selection is the process of selecting a subset of relevant features (variables or predictors). It is a critical step that can have a profound impact on the performance of your model. The main benefits of feature selection include:\n",
    "\n",
    "- Simpler Models\n",
    "- Less Overfitting\n",
    "- Speed up Training\n",
    "- Improved Accuranccy\n",
    "- Reduces Noise\n",
    "- Prevents Multicolinearity\n",
    "\n",
    "\n",
    "It's also important to remember that feature importance doesn't necessarily imply causality; a feature may be important in the context of a particular model without being a causal factor for the outcome being predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_significant_features(X_train, X_test, y_train, n):\n",
    "    # Feature selection using Extra Trees Classifier on the resampled training data\n",
    "    model = ExtraTreeClassifier(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    importances = model.feature_importances_\n",
    "    importances_normalized = np.std([tree.feature_importances_ for tree in\n",
    "                                        model.estimators_],\n",
    "                                        axis = 0)\n",
    "\n",
    "    # Select top features with highest importance scores\n",
    "    top_features = pd.Series(importances, index=X_train.columns).nlargest(n)\n",
    "\n",
    "    # Subset X_resampled and X_test with selected features\n",
    "    X_train_selected = X_train[top_features.index]\n",
    "    X_test_selected = X_test[top_features.index]\n",
    "\n",
    "    return X_train_selected, X_test_selected, importances_normalized\n",
    "\n",
    "\n",
    "def random_forest_feature_selection(X_train, X_test, y_train, n):\n",
    "    \n",
    "    model = SelectFromModel(RandomForestClassifier(n_estimators = n))\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    list_train_rf= X_train.columns[(model.get_support())]\n",
    "    list_test_rf= X_test.columns[(model.get_support())]\n",
    "\n",
    "    X_train_rf = X_train[list_train_rf]\n",
    "    X_test_rf = X_test[list_test_rf]\n",
    "    \n",
    "    return X_train_rf, X_test_rf\n",
    "\n",
    "def xg_boost_feature_selection(X_train, X_test, y_train):\n",
    "    \n",
    "    params = { \"objective\": \"multi:softmax\", 'num_class': 2 , 'random_state': 42 }\n",
    "    model= xgb.XGBClassifier(**params)\n",
    "    select_xgbc = SelectFromModel(estimator = model, threshold = \"median\")\n",
    "    select_xgbc.fit(X_train, y_train)\n",
    "\n",
    "    list_train_xgbc= X_train.columns[(select_xgbc.get_support())]\n",
    "    list_test_xgbc= X_test.columns[(select_xgbc.get_support())]\n",
    "\n",
    "\n",
    "    X_train_xgbc = X_train[list_train_xgbc]\n",
    "    X_test_xgbc = X_test[list_test_xgbc]\n",
    "    \n",
    "    return X_train_xgbc, X_test_xgbc\n",
    "\n",
    "def pca_feature_selection(X_train, X_test, y_train):\n",
    "\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Create a PCA object and fit it to the scaled data\n",
    "    pca = PCA(n_components=3) # Select the number of components you want to keep\n",
    "    pca.fit(X_train_scaled)\n",
    "\n",
    "    # Transform the data to the selected number of components\n",
    "    X_train_pca = pca.transform(X_train_scaled)\n",
    "    X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "    return X_train_pca, X_test_pca"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Classifiaction/ Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print proportions of each class in cleaned data set\n",
    "X_combined = pd.concat([X_train, X_val], axis=0, ignore_index=True)\n",
    "y_combined =  pd.concat([y_train, y_val], axis =0, ignore_index=True)\n",
    "\n",
    "sell_prop_trainc = sum(y_combined == 0)/y_combined.shape[0]\n",
    "buy_prop_trainc = sum(y_combined == 1)/y_combined.shape[0]\n",
    "\n",
    "print(f\"Sell proportion:  {sell_prop_trainc: .6f}\")\n",
    "print(f\"Buy proportion:  {buy_prop_trainc: .6f}\")\n",
    "\n",
    "\n",
    "############## Naive Classification/ Baseline ##############\n",
    "priors = [sell_prop_trainc,  buy_prop_trainc]\n",
    "np.random.seed(42) # set seed\n",
    "\n",
    "# randomly choose classes 0, 1, with probabilities based on proportions from X_train_cleaned\n",
    "y_pred = np.random.choice([0, 1], size = len(y_test), replace = True, p = priors) \n",
    "\n",
    "f1_weighted = metrics.f1_score(y_test, y_pred, average = \"weighted\")\n",
    "print(f\"Test Score:  {f1_weighted: .6f}\")\n",
    "final_report(y_test, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.qda import QDA\n",
    "# QDA-Modell erstellen und anpassen\n",
    "qda = QuadraticDiscriminantAnalysis(priors=priors)\n",
    "qda.fit(X_combined, y_combined)\n",
    "\n",
    "# Vorhersagen auf Testdaten machen\n",
    "y_pred = qda.predict(X_test)\n",
    "\n",
    "final_report(y_test, y_pred)\n",
    "plot_curves_roc( qda ,X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Logistic Regression without Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression( random_state=42, max_iter= 10000, n_jobs=-1) \n",
    "## Default Parameters set as:\n",
    "#LogisticRegression(penalty='l2', C=1.0,  class_weight=None,  solver='lbfgs', max_iter=100)\n",
    "\n",
    "pipeline_params = {'ros': [None ], # smote_nc for categorical data\n",
    "              'scaler': [scaler, mms], # have to scale????\n",
    "            } \n",
    "\n",
    "pipe = imbpipeline(steps=[[\"scaler\", scaler], [\"ros\", ros], [\"model\", model]])\n",
    "\n",
    "train = TrainModel(X_train, y_train, X_test, y_test, X_val, y_val, model=model, pipeline =pipe, model_params=None, pipeline_params=pipeline_params)\n",
    "train.train_with_crossvalidation(use_pipeline=True, use_grid_search=False) \n",
    "train.evaluate_model()\n",
    "#train.analyse_feature_importance_all(number_of_sample=100)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression( random_state=42, max_iter= 10000, n_jobs=-1) \n",
    "\n",
    "\n",
    "pipeline_params = {'ros': [None, ros, smote, borderline_smote , smote_nc], # smote_nc for categorical data\n",
    "              'scaler': [scaler, mms], # have to scale????\n",
    "              'model__solver': ['saga'], # 'lbfgs', 'newton-cholesky'\n",
    "              'model__penalty': ['l2',  None], # Removed None since 'l2' is the default and None would be redundant, 'l1'\n",
    "              'model__class_weight': ['balanced', None],\n",
    "              'model__C': [ 0.01, 0.1, 1, 10, 100],\n",
    "            } \n",
    "\n",
    "pipe = imbpipeline(steps=[[\"scaler\", scaler], [\"ros\", ros], [\"model\", model]])\n",
    "\n",
    "train = TrainModel(X_train, y_train, X_test, y_test, X_val, y_val,  model=model, pipeline =pipe, model_params=None, pipeline_params=pipeline_params)\n",
    "train.train_with_validation(use_pipeline=True, use_grid_search=False) \n",
    "train.evaluate_model()\n",
    "#train.plot_learning_curve()\n",
    "#train.analyse_feature_importance_all(number_of_sample=100) #shap wont work!!!!\n",
    "#train.permutation_importance_plot()\n",
    "#train.linear_model_coefficients()\n",
    "#train.correlation_importance()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision-Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing decision tree\n",
    "model = DecisionTreeClassifier(random_state = 42)\n",
    "#Defult Parameters:\n",
    "#DecisionTreeClassifier( criterion='gini', splitter='best', max_depth=None,  max_features=None, class_weight=None,)\n",
    "\n",
    "# hyperparameter to be tested\n",
    "pipeline_params = {\n",
    "    'ros': [None, ros, smote, borderline_smote, smote_nc],\n",
    "    \"model__criterion\": ['gini', 'entropy'],\n",
    "    \"model__splitter\": ['best', 'random'],\n",
    "    \"model__max_features\": [None, \"auto\", \"sqrt\", \"log2\"],\n",
    "    \"model__class_weight\": [None, \"balanced\", \"balanced_subsample\"],\n",
    "}\n",
    "\n",
    "pipe = imbpipeline(steps=[[\"ros\", ros], [\"model\", model]]) \n",
    "\n",
    "train = TrainModel(X_train, y_train, X_test, y_test, X_val, y_val, model=model, pipeline =pipe, pipeline_params=pipeline_params)\n",
    "train.train_with_crossvalidation(use_pipeline=True,  use_grid_search=False)  \n",
    "train.evaluate_model()\n",
    "#train.plot_learning_curve()\n",
    "train.analyse_feature_importance_all(number_of_sample=100)\n",
    "\n",
    "\n",
    "# Plot tree\n",
    "clf_temp = DecisionTreeClassifier(max_depth=4, random_state = 42)\n",
    "clf_temp = clf_temp.fit(X_train, y_train)\n",
    "plt.figure(figsize=(40, 23))\n",
    "plot_tree(clf_temp, filled=True, feature_names = list(X_train.columns), rounded=True, class_names=[\"sell\", \"buy\"])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ExtraTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = X_train.select_dtypes(include='category').columns.tolist()\n",
    "print(categorical_features)\n",
    "\n",
    "\n",
    "model = ExtraTreesClassifier(random_state = 42, n_jobs=-1) #####TreeSSS\n",
    "## Defult Parameter:\n",
    "# ExtraTreesClassifier(n_estimators=100, criterion='gini', max_depth=None, max_features='sqrt', class_weight=None,  max_samples=None)\n",
    "\n",
    "pipeline_params = {\n",
    "    'ros': [None, ros, smote, borderline_smote, smote_nc],\n",
    "    \"model__criterion\": [\"gini\", \"entropy\"],\n",
    "    \"model__max_features\": [ None, \"sqrt\", \"log2\"],\n",
    "    \"model__n_estimators\": [50, 100, 200, 250],\n",
    "    \"model__class_weight\": [None, \"balanced\", \"balanced_subsample\"],\n",
    "}\n",
    "\n",
    "pipe = imbpipeline(steps=[[\"ros\", ros], [\"model\", model]]) \n",
    "\n",
    "train = TrainModel(X_train, y_train, X_test, y_test, X_val, y_val, model=model, pipeline =pipe, pipeline_params=pipeline_params)\n",
    "train.train_with_crossvalidation(use_pipeline=True,  use_grid_search=False) \n",
    "train.evaluate_model()\n",
    "#train.plot_learning_curve()\n",
    "#train.analyse_feature_importance_all(number_of_sample=100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(random_state = 42, n_jobs=-1)\n",
    "\n",
    "\n",
    "pipeline_params = {\n",
    "    'ros': [None, ros, smote, borderline_smote, smote_nc],\n",
    "    \"model__criterion\": [\"gini\", \"entropy\"],\n",
    "    \"model__max_features\": [ None, \"sqrt\", \"log2\"],\n",
    "    \"model__n_estimators\": [ 50, 100, 200, 250],\n",
    "    \"model__class_weight\": [None, \"balanced\", \"balanced_subsample\"],\n",
    "}\n",
    "\n",
    "\n",
    "pipe = imbpipeline(steps=[[\"ros\", ros], [\"model\", model]]) \n",
    "\n",
    "train = TrainModel(X_train, y_train, X_test, y_test, X_val, y_val, model=model, pipeline =pipe, pipeline_params=pipeline_params)\n",
    "train.train_with_crossvalidation(use_pipeline=True,  use_grid_search=True) \n",
    "train.evaluate_model()\n",
    "#train.plot_learning_curve()\n",
    "train.analyse_feature_importance_all(number_of_sample=100)\n",
    "\n",
    "## Defult Parameters\n",
    "# RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=None, max_features='sqrt',  class_weight=None,  max_samples=None)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Desecent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SGDClassifier(random_state = 42, max_iter = 1000, n_jobs =-1)\n",
    "## Defult Parameters\n",
    "#SGDClassifier(loss='hinge', penalty='l2', alpha=0.0001, l1_ratio=0.15,  max_iter=1000,  learning_rate='optimal',   class_weight=None,)\n",
    "\n",
    "pipeline_params = {\n",
    "    'ros': [None, ros, smote, borderline_smote, smote_nc],\n",
    "    'scaler': [scaler, None, mms], \n",
    "    \"model__loss\": [ 'log_loss', 'modified_huber'], #'hinge',\n",
    "    \"model__penalty\": ['l2', 'l1', 'elasticnet'],\n",
    "}\n",
    "\n",
    "\n",
    "pipe = imbpipeline(steps=[[\"ros\", ros], [\"scaler\", scaler], [\"model\", model]]) \n",
    "\n",
    "train = TrainModel(X_train, y_train, X_test, y_test, X_val, y_val, model=model, pipeline =pipe, pipeline_params=pipeline_params)\n",
    "train.train_with_validation(use_pipeline=True,  use_grid_search=False) \n",
    "train.evaluate_model()\n",
    "#train.plot_learning_curve()\n",
    "#train.analyse_feature_importance_all(number_of_sample=100)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AdaBoostClassifier(random_state=42)\n",
    "#Deafult Parameters\n",
    "#AdaBoostClassifier(estimator=None,  n_estimators=50, learning_rate=1.0, algorithm='SAMME.R',  base_estimator='deprecated')\n",
    "\n",
    "pipeline_params = {\n",
    "    'ros': [None, ros, smote, borderline_smote, smote_nc],\n",
    "    'scaler': [scaler, None, mms], \n",
    "    \"model__learning_rate\": [0.01, 0.05, 0.1, 0.3],\n",
    "    \"model__n_estimators\": [50, 100, 200, 250],\n",
    "    \"model__algorithm\": ['SAMME', 'SAMME.R'],\n",
    "}\n",
    "\n",
    "\n",
    "pipe = imbpipeline(steps=[[\"ros\", ros], [\"scaler\", scaler],[\"model\", model]]) \n",
    "\n",
    "train = TrainModel(X_train, y_train, X_test, y_test, X_val, y_val, model=model, pipeline =pipe, pipeline_params=pipeline_params)\n",
    "train.train_with_validation(use_pipeline=True, use_grid_search=False)  \n",
    "train.evaluate_model()\n",
    "#train.plot_learning_curve()\n",
    "#train.analyse_feature_importance_all(number_of_sample=100)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(objective = \"binary:logistic\",  enable_categorical=True, random_state = 42, n_jobs =-1) \n",
    "\n",
    "pipeline_params = {\n",
    "    'ros': [None, ros, smote, borderline_smote, smote_nc], # \n",
    "    \"model__booster\": ['gbtree', 'dart'], #'gblinear'\n",
    "    \"model__max_depth\": [3, 6, 10 ],\n",
    "    \"model__learning_rate\": [ 0.1, 0.3],\n",
    "}\n",
    "\n",
    "\n",
    "pipe = imbpipeline(steps=[[\"ros\", ros], [\"model\", model]]) \n",
    "\n",
    "train = TrainModel(X_train, y_train, X_test, y_test, X_val, y_val, model=model, pipeline =pipe, pipeline_params=pipeline_params)\n",
    "train.train_with_crossvalidation(use_pipeline=True, use_grid_search=True) \n",
    "train.evaluate_model()\n",
    "#train.plot_learning_curve()\n",
    "train.analyse_feature_importance_all(number_of_sample=100)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = X_train.select_dtypes(include='category').columns.tolist()\n",
    "print(categorical_features)\n",
    "\n",
    "X_train_to_use = X_train.copy(deep=True)\n",
    "X_test_to_use = X_test.copy(deep=True)\n",
    "X_val_to_use = X_val.copy(deep=True)\n",
    "\n",
    "## Convert to int or string\n",
    "for cat in categorical_features:\n",
    "        X_train_to_use[cat] = X_train_to_use[cat].astype('string')\n",
    "        X_test_to_use[cat] = X_test_to_use[cat].astype('string')\n",
    "        X_val_to_use[cat] = X_val_to_use[cat].astype('string')  \n",
    "\n",
    "\n",
    "model= CatBoostClassifier(cat_features=categorical_features, loss_function=\"Logloss\", random_state = 42, verbose= 500)\n",
    "\n",
    "\n",
    "pipeline_params = {'ros': [ros, smote_nc, None], # smote, borderline_smote,\n",
    "                   'model__learning_rate': [0.05, 0.1],\n",
    "                   'model__depth': [10, 15, 20],\n",
    "                   'model__iterations': [1000]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "pipe = imbpipeline(steps=[ [\"ros\", ros], [\"model\", model]])\n",
    "\n",
    "train = TrainModel(X_train_to_use, y_train, X_test_to_use, y_test, X_val_to_use, y_val, model=model, pipeline =pipe, model_params=None, pipeline_params=pipeline_params)\n",
    "train.train_with_validation(use_pipeline=True,  use_grid_search=False) \n",
    "train.evaluate_model()\n",
    "#train.plot_learning_curve()\n",
    "train.analyse_feature_importance_all(number_of_sample=100)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= SVC(random_state=42, probability=True, max_iter= -1) #probability=True enables probabiliry estimates.\n",
    "## Default Parameters\n",
    "#SVC(C=1.0, kernel='rbf', degree=3, gamma='scale',  cache_size=200, class_weight=None,  max_iter=-1)\n",
    "\n",
    "pipeline_params = {\n",
    "    'ros': [None, ros , smote, borderline_smote, smote_nc], \n",
    "    'scaler': [scaler, mms],\n",
    "    \"model__kernel\": [\"rbf\", \"sigmoid\"], #\"linear\"\n",
    "    'model__gamma': [\"auto\", \"scale\"],\n",
    "    'model__class_weight': [None, 'balanced'], \n",
    "    'model__C': [ 0.05, 1.0, 5.0, 10.0, 20.0 , 50.0],\n",
    "}\n",
    "\n",
    "pipe = imbpipeline(steps=[[\"ros\", ros],[\"scaler\", scaler], [\"model\", model]])\n",
    "\n",
    "train = TrainModel(X_train, y_train, X_test, y_test, X_val, y_val, model=model, pipeline =pipe, pipeline_params=pipeline_params)\n",
    "train.train_with_crossvalidation(use_pipeline=True,  use_grid_search=False)  \n",
    "train.evaluate_model()\n",
    "#train.plot_learning_curve()\n",
    "train.analyse_feature_importance_all(number_of_sample=100)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianNB()\n",
    "\n",
    "y_combined =  pd.concat([y_train, y_val], axis =0).reset_index(drop=True)\n",
    "priors = (sum(y_combined == 0)/y_combined.shape[0], sum(y_combined == 1)/y_combined.shape[0])\n",
    "\n",
    "pipeline_params = {\n",
    "    'ros': [None, ros, smote, borderline_smote, smote_nc],\n",
    "    \"scaler\": [scaler, mms, None],\n",
    "    \"model__priors\": [None, priors]\n",
    "}\n",
    "\n",
    "pipe = imbpipeline(steps=[[\"scaler\", scaler], [\"ros\", ros], [\"model\", model]])\n",
    "\n",
    "train = TrainModel(X_train, y_train, X_test, y_test, X_val, y_val, model=model, pipeline =pipe, pipeline_params=pipeline_params)\n",
    "train.train_with_validation(use_pipeline=True,  use_grid_search=False) \n",
    "train.evaluate_model()\n",
    "#train.plot_learning_curve()\n",
    "#train.analyse_feature_importance_all(number_of_sample=100)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()\n",
    "\n",
    "y_combined =  pd.concat([y_train, y_val], axis =0, ignore_index=True)\n",
    "priors = (sum(y_combined == 0)/y_combined.shape[0], sum(y_combined == 1)/y_combined.shape[0])\n",
    "\n",
    "pipeline_params = {\n",
    "    'ros': [None, ros, smote, borderline_smote, smote_nc],\n",
    "    \"scaler\": [scaler, mms, None],\n",
    "    \"model__class_prior\": [None, priors],\n",
    "    \"model__force_alpha\": [ True, False]\n",
    "}\n",
    "\n",
    "pipe = imbpipeline(steps=[[\"scaler\", scaler], [\"ros\", ros], [\"model\", model]])\n",
    "\n",
    "train = TrainModel(X_train, y_train, X_test, y_test, X_val, y_val, model=model, pipeline =pipe, pipeline_params=pipeline_params)\n",
    "train.train_with_validation(use_pipeline=True, use_grid_search=False) \n",
    "train.evaluate_model()\n",
    "#train.plot_learning_curve()\n",
    "#train.analyse_feature_importance_all(number_of_sample=100)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H20 Package Missing Data Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2o\n",
    "from h2o.estimators import H2ORandomForestEstimator\n",
    "\n",
    "from h2o.transforms.decomposition import H2OPrincipalComponentAnalysisEstimator\n",
    "from h2o.estimators import H2OGradientBoostingEstimator\n",
    "\n",
    "\n",
    "def initialize_h2o():\n",
    "    h2o.init()\n",
    "\n",
    "def prepare_data(df, X):\n",
    "    df_h2o = h2o.H2OFrame(df)\n",
    "    response = 'Target'\n",
    "    df_h2o[response] = df_h2o[response].asfactor()\n",
    "    predictors = X.columns.tolist()\n",
    "    train, valid, test = df_h2o.split_frame(ratios=[.8, .1], seed=42) #TODO can you do this split so its comparabal to all other methods in the class TrainModel \n",
    "    return train, valid, test, predictors, response\n",
    "\n",
    "def train_model(train, valid, predictors, response):\n",
    "    model = H2ORandomForestEstimator(ntrees=50, max_depth=20, nfolds=15, binomial_double_trees=False) # binomial_double_trees=True is not supported for SHAP!!!!\n",
    "    model.train(x=predictors, y=response, training_frame=train, validation_frame=valid)\n",
    "    return model\n",
    "\n",
    "def plot_performance_curves(performance):\n",
    "    fpr = performance.fprs\n",
    "    tpr = performance.tprs\n",
    "    roc_auc = performance.auc()\n",
    "    precision = performance.precision()[0]\n",
    "    recall = performance.recall()[0]\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    ax1.plot(fpr, tpr, color='b', label=f'AUC = {roc_auc:.2f}')\n",
    "    ax1.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "    ax1.set_xlim([0.0, 1.0])\n",
    "    ax1.set_ylim([0.0, 1.05])\n",
    "    ax1.set_xlabel('False Positive Rate')\n",
    "    ax1.set_ylabel('True Positive Rate')\n",
    "    ax1.set_title('ROC Curve')\n",
    "    ax1.legend(loc=\"lower right\")\n",
    "\n",
    "    ax2.plot(recall, precision, color='b')\n",
    "    ax2.set_xlim([0.0, 1.0])\n",
    "    ax2.set_ylim([0.0, 1.05])\n",
    "    ax2.set_xlabel('Recall')\n",
    "    ax2.set_ylabel('Precision')\n",
    "    ax2.set_title('Precision-Recall Curve')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(performance):\n",
    "    confusion_matrix_h2o = performance.confusion_matrix()\n",
    "    cm_data = pd.DataFrame(confusion_matrix_h2o.to_list())\n",
    "    cm_data = cm_data.iloc[:2, :2]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm_data, annot=True, fmt=\".0f\", cmap=\"Blues\")\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "def plot_feature_importance(model):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    importance = model.varimp(use_pandas=True)\n",
    "    sns.barplot(x='scaled_importance', y='variable', data=importance)\n",
    "    plt.title('Feature Importance')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def random_forest_classification_h2o(df, X, y):\n",
    "    initialize_h2o()\n",
    "\n",
    "    train, valid, test, predictors, response = prepare_data(df, X)\n",
    "    model = train_model(train, valid, predictors, response)\n",
    "    performance = model.model_performance(test_data=test)\n",
    "    print(performance)\n",
    "\n",
    "    plot_performance_curves(performance)\n",
    "    plot_confusion_matrix(performance)\n",
    "    plot_feature_importance(model)\n",
    "\n",
    "    # Use the built-in H2O function to plot feature importance\n",
    "    model.varimp_plot()\n",
    "\n",
    "\n",
    "    predictions = model.predict(test)\n",
    "    h2o.shutdown(prompt=False)\n",
    "\n",
    "\n",
    "\n",
    "random_forest_classification_h2o(mydata.data, mydata.X, mydata.y)\n",
    "\n",
    "\n",
    "### TODO apply SHAP Feature Importance for H20 Packages see the function shap in TrainModel() class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 3D plots, we will need Axes3D\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from PyALE import ale\n",
    "\n",
    "\n",
    "def plot_pdp(model, data, feature, nbins=20):\n",
    "    \"\"\"\n",
    "    Plot PDP for a specific feature.\n",
    "    \"\"\"\n",
    "    pdp = model.partial_plot(data, cols=[feature], nbins=nbins, plot=True)\n",
    "\n",
    "def plot_ale(model, data, feature):\n",
    "    # Predict function for your model\n",
    "    def predict_fn(X):\n",
    "        df = h2o.H2OFrame(X)\n",
    "        pred = model.predict(df).as_data_frame().values\n",
    "        return pred\n",
    "    \n",
    "    # Get numpy data without target\n",
    "    X = data.drop('Target', axis=1).as_data_frame().values\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ale(predict_fn, X, feature, ax=ax)\n",
    "    ax.set_title(f\"ALE plot for {feature}\")\n",
    "    plt.show()\n",
    "    \n",
    "def plot_interaction_heatmap(model, data, feature1, feature2):\n",
    "    \"\"\"\n",
    "    Plot two-way interaction heatmap.\n",
    "    \"\"\"\n",
    "    pdp = model.partial_plot(data, cols=[feature1, feature2], plot=False)\n",
    "    interaction_data = pdp[0].as_data_frame()\n",
    "    pivoted_data = interaction_data.pivot(index=feature1, columns=feature2, values='mean_response')\n",
    "    sns.heatmap(pivoted_data, cmap='YlGnBu', annot=True)\n",
    "    plt.title(f\"Interaction between {feature1} and {feature2}\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_interaction_3d(model, data, feature1, feature2, feature3):\n",
    "    \"\"\"\n",
    "    Plot three-way interaction in 3D.\n",
    "    \"\"\"\n",
    "    pdp = model.partial_plot(data, cols=[feature1, feature2, feature3],  plot=False)\n",
    "    interaction_data = pdp[0].as_data_frame()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(interaction_data[feature1], interaction_data[feature2], interaction_data[feature3], c=interaction_data['mean_response'], cmap='YlGnBu')\n",
    "    ax.set_xlabel(feature1)\n",
    "    ax.set_ylabel(feature2)\n",
    "    ax.set_zlabel(feature3)\n",
    "    ax.set_title(f\"Interaction among {feature1}, {feature2}, and {feature3}\")\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods with Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logisitc Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression( random_state=42, max_iter= 10000, n_jobs=-1)  #solver='lbfgs'\n",
    "\n",
    "\n",
    "pipeline_params = {'ros': [None, ros, smote, borderline_smote, smote_nc], # upsampling or not\n",
    "              'scaler': [scaler, None, mms], # scaling input by standardizing or min-max scaling or not scaling at all\n",
    "              'model__solver': ['lbfgs', 'saga'],\n",
    "              'model__multi_class': ['auto'],\n",
    "              'model__penalty': ['l2', 'l1', 'elasticnet'], # Removed None since 'l2' is the default and None would be redundant\n",
    "            } \n",
    "\n",
    "\n",
    "pipe = imbpipeline(steps=[[\"scaler\", scaler], [\"ros\", ros], [\"model\", model]])\n",
    "\n",
    "train = TrainModel(X_train, y_train, X_test, y_test, X_val, y_val, model=model, pipeline =pipe, model_params=None, pipeline_params=pipeline_params)\n",
    "train.train_with_validation(use_pipeline=True, use_grid_search=False) \n",
    "train.evaluate_model()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with Random Forest Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classifier and its parameters\n",
    "model = LogisticRegression(solver='saga', random_state=42, max_iter= 1000, n_jobs=-1)\n",
    "# Define feature selector and its parameters\n",
    "feat_sel = RandomForestClassifier(random_state = 42, n_estimators=100, n_jobs=-1)\n",
    "\n",
    "# Construct the pipeline\n",
    "pipe = imbpipeline(steps=[[\"scaler\", scaler], [\"feature_selection\",  SelectFromModel(estimator=feat_sel, threshold=\"median\")],\n",
    "                          [\"ros\", ros], [\"model\", model]])\n",
    "\n",
    "\n",
    "train = TrainModel(X_train, y_train, X_test, y_test, X_val, y_val, model=model, pipeline =pipe, pipeline_params=pipeline_params)\n",
    "train.train_with_validation(use_pipeline=True,  use_grid_search=False) \n",
    "train.evaluate_model()\n",
    "train.selected_features()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with XGBoost Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classifier and its parameters\n",
    "model = LogisticRegression(solver='saga', random_state=42, max_iter= 1000, n_jobs=-1)\n",
    "# Define feature selector and its parameters\n",
    "feat_sel = xgb.XGBClassifier(objective = \"binary:logistic\", enable_categorical=True, random_state = 42, n_jobs =-1)\n",
    "# Construct the pipeline\n",
    "pipe = imbpipeline(steps=[[\"scaler\", scaler], [\"feature_selection\",  SelectFromModel(estimator=feat_sel, threshold=\"median\")],\n",
    "                          [\"ros\", ros], [\"model\", model]])\n",
    "\n",
    "\n",
    "train = TrainModel(X_train, y_train, X_test, y_test, X_val, y_val, model=model, pipeline =pipe, pipeline_params=pipeline_params)\n",
    "train.train_with_validation(use_pipeline=True,  use_grid_search=False) \n",
    "train.evaluate_model()\n",
    "train.selected_features()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with PCA Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classifier and its parameters\n",
    "model = LogisticRegression(solver='saga', random_state=42, max_iter= 1000, n_jobs=-1)\n",
    "\n",
    "pca = KernelPCA(random_state = 42, eigen_solver = \"arpack\")\n",
    "\n",
    "pipeline_params = { \n",
    "    'feature_selection__n_components': np.arange(5, 10, 1),\n",
    "    \"feature_selection__kernel\": [\"linear\", \"sigmoid\"],\n",
    "    'model__penalty': ['l2', 'l1', 'elasticnet'],\n",
    "    'ros': [None, ros, smote, borderline_smote, smote_nc],\n",
    "    'scaler': [scaler, mms], #scaling since PCA?\n",
    "}\n",
    "\n",
    "# Construct the pipeline\n",
    "pipe = imbpipeline(steps=[[\"scaler\", scaler], [\"feature_selection\",  pca],\n",
    "                          [\"ros\", ros], [\"model\", model]])\n",
    "\n",
    "train = TrainModel(X_train, y_train, X_test, y_test, X_val, y_val, model=model, pipeline =pipe, pipeline_params=pipeline_params)\n",
    "train.train_with_validation(use_pipeline=True,  use_grid_search=False)  \n",
    "train.evaluate_model()\n",
    "train.pca_components()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Random Forest\n",
    "\n",
    " Random Forest is a popular and versatile machine learning method that is capable of performing both regression and classification tasks. It is also used for dimensionality reduction, treats missing values, outlier values, and other things.\n",
    "\n",
    " Random Forests generally have a high prediction accuracy and are quite efficient on large datasets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest without Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(random_state = 42, n_jobs=-1)\n",
    "\n",
    "pipeline_params = {\n",
    "    \"model__criterion\": [\"gini\", \"entropy\"],\n",
    "    \"model__max_features\": [ \"sqrt\", \"log2\", None],\n",
    "    \"model__n_estimators\": np.array([ 50, 100, 200, 250]),\n",
    "    \"model__class_weight\": [None, \"balanced\", \"balanced_subsample\"],\n",
    "    \"model__max_depth\": np.array([None, 5, 10, 20]),\n",
    "    'ros': [None, ros, smote, borderline_smote, smote_nc],\n",
    "}\n",
    "\n",
    "pipe = imbpipeline(steps=[[\"ros\", ros], [\"model\", model]]) \n",
    "\n",
    "train = TrainModel(X_train, y_train, X_test, y_test, X_val, y_val, model=model, pipeline =pipe, pipeline_params=pipeline_params)\n",
    "train.train_with_validation(use_pipeline=True,  use_grid_search=False) \n",
    "train.evaluate_model()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest with Random Forest Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_sel = RandomForestClassifier(random_state = 42, n_estimators=100, n_jobs=-1)\n",
    "pipe = imbpipeline(steps=[[\"feature_selection\",  SelectFromModel(estimator = feat_sel, threshold = \"median\")],\n",
    "                          [\"ros\", ros], [\"model\", model]])\n",
    "\n",
    "train = TrainModel(X_train, y_train, X_test, y_test, X_val, y_val, model=model, pipeline =pipe, pipeline_params=pipeline_params)\n",
    "train.train_with_validation(use_pipeline=True,  use_grid_search=False) \n",
    "train.evaluate_model()\n",
    "train.selected_features()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Random Forest with XGBoost Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_sel = xgb.XGBClassifier(objective = \"binary:logistic\", enable_categorical=True, random_state = 42, n_jobs =-1)\n",
    "pipe = imbpipeline(steps=[[\"feature_selection\",  SelectFromModel(estimator = feat_sel, threshold = \"median\")],\n",
    "                          [\"ros\", ros], [\"model\", model]])\n",
    "\n",
    "train = TrainModel(X_train, y_train, X_test, y_test, X_val, y_val, model=model, pipeline =pipe, pipeline_params=pipeline_params)\n",
    "train.train_with_validation(use_pipeline=True, use_grid_search=False) \n",
    "train.evaluate_model()\n",
    "train.selected_features()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest with PCA Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(random_state = 42)\n",
    "\n",
    "pca = KernelPCA(random_state = 42, eigen_solver = \"arpack\")\n",
    "\n",
    "pipeline_params = { \"model__criterion\": [\"gini\", \"entropy\"],\n",
    "                    \"model__max_features\": [ \"sqrt\", \"log2\"],\n",
    "                    \"model__max_depth\": np.array([None, 5, 10, 20]),\n",
    "                    \"model__n_estimators\": np.array([ 50, 100, 200, 250]),\n",
    "                    \"model__class_weight\": [None, \"balanced\", \"balanced_subsample\"],\n",
    "                    \"feature_selection__kernel\": [\"linear\", \"sigmoid\"],\n",
    "                    'feature_selection__n_components': np.arange(5, 10, 1),\n",
    "                    'ros': [None, ros, smote, borderline_smote, smote_nc],\n",
    "                    'scaler': [ mms, scaler] ### Have to scale for PCA right?\n",
    "}\n",
    "    \n",
    "\n",
    "pipe = imbpipeline(steps=[[\"scaler\", scaler], [\"feature_selection\", pca ],  [\"ros\", ros], [\"model\", model]])\n",
    "\n",
    "train = TrainModel(X_train, y_train, X_test, y_test, X_val, y_val, model=model, pipeline =pipe, pipeline_params=pipeline_params)\n",
    "train.train_with_validation(use_pipeline=True, use_grid_search=False) \n",
    "train.evaluate_model()\n",
    "train.pca_components()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xg-Boost\n",
    "\n",
    "### XgB without Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(objective =\"binary:logistic\", enable_categorical=True, random_state = 42, n_jobs =-1)\n",
    "\n",
    "pipeline_params = {\n",
    "    \"model__booster\": ['gbtree', 'dart'], \n",
    "    \"model__max_depth\": [3, 6, 10],\n",
    "    \"model__learning_rate\": [ 0.1, 0.3],\n",
    "    'ros': [None, ros, smote_nc], #smote, borderline_smote, \n",
    "}\n",
    "\n",
    "\n",
    "pipe = imbpipeline(steps=[[\"ros\", ros], [\"model\", model]]) \n",
    "\n",
    "train = TrainModel(X_train, y_train, X_test, y_test, X_val, y_val, model=model, pipeline =pipe, pipeline_params=pipeline_params)\n",
    "train.train_with_validation(use_pipeline=True,  use_grid_search=False) \n",
    "train.evaluate_model()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xbg with Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_sel = RandomForestClassifier(random_state = 42, n_estimators=100, n_jobs=-1)\n",
    "\n",
    "pipe = imbpipeline(steps=[[\"feature_selection\",  SelectFromModel(estimator=feat_sel, threshold=\"median\")],\n",
    "                          [\"ros\", ros], [\"model\", model]])\n",
    "\n",
    "train = TrainModel(X_train, y_train, X_test, y_test, X_val, y_val, model=model, pipeline =pipe, pipeline_params=pipeline_params)\n",
    "train.train_with_validation(use_pipeline=True, use_grid_search=False) \n",
    "train.evaluate_model()\n",
    "train.selected_features()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xbg with XBg Featrue Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_sel = xgb.XGBClassifier(objective =\"binary:logistic\", enable_categorical=True, random_state = 42, n_jobs =-1)\n",
    "pipe = imbpipeline(steps=[[\"feature_selection\",  SelectFromModel(estimator = feat_sel, threshold = \"median\")],\n",
    "                          [\"ros\", ros], [\"model\", model]])\n",
    "\n",
    "train = TrainModel(X_train, y_train, X_test, y_test, X_val, y_val,  model=model, pipeline =pipe, pipeline_params=pipeline_params)\n",
    "train.train_with_validation(use_pipeline=True, use_grid_search=False) \n",
    "train.evaluate_model()\n",
    "train.selected_features()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = KernelPCA(random_state = 42, eigen_solver = \"arpack\")\n",
    "#pca = PCA()\n",
    "pipeline_params = { \"model__booster\": ['gbtree', 'dart'], \n",
    "    \"model__max_depth\": [3, 6, 10],\n",
    "    \"model__learning_rate\": [ 0.1, 0.3],\n",
    "    'ros': [None, ros, smote_nc], #smote, borderline_smote,\n",
    "    'scaler': [ mms, scaler], ### have to scale since PCA right?\n",
    "    \"feature_selection__kernel\": [\"linear\", \"sigmoid\"],\n",
    "    'feature_selection__n_components': np.arange(5, 10, 1),  \n",
    "}\n",
    "    \n",
    "\n",
    "pipe = imbpipeline(steps=[[\"scaler\", scaler], [\"feature_selection\", pca ],  [\"ros\", ros], [\"model\", model]])\n",
    "\n",
    "train = TrainModel(X_train, y_train, X_test, y_test, X_val , y_val,  model=model, pipeline =pipe, pipeline_params=pipeline_params)\n",
    "train.train_with_validation(use_pipeline=True,  use_grid_search=False) \n",
    "train.evaluate_model()\n",
    "train.pca_components()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "Support Vector Machines (SVMs) aim to find a distinct hyperplane in a high-dimensional space to classify data points. They excel in managing high-dimensional data, even when dimensions outnumber samples, and are memory-efficient by leveraging a subset of training points called \"support vectors\" in decision-making.\n",
    "\n",
    "However, SVMs struggle with large feature sets compared to samples, requiring careful kernel and regularization term selection to avoid over-fitting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM without Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= SVC(random_state=42, probability=True, max_iter= 1000)\n",
    "\n",
    "pipeline_params = {\n",
    "    'ros': [ros, smote, borderline_smote, smote_nc, None], \n",
    "    \"model__kernel\": [\"linear\", \"rbf\"],\n",
    "    'ros': [None, ros, smote, borderline_smote, smote_nc],\n",
    "    'scaler': [ mms, scaler]\n",
    "}\n",
    "\n",
    "pipe = imbpipeline(steps=[[\"ros\", ros], [\"scaler\", scaler],[\"model\", model]])\n",
    "\n",
    "train = TrainModel(X_train, y_train, X_test, y_test, X_val, y_val, model=model, pipeline =pipe, pipeline_params=pipeline_params)\n",
    "train.train_with_validation(use_pipeline=True,  use_grid_search=False) \n",
    "train.evaluate_model()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  SVM with RandomForest Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature selector and its parameters\n",
    "model= SVC(random_state=42,probability=True, max_iter= 1000)\n",
    "feat_sel = RandomForestClassifier(random_state = 42, n_estimators=100, n_jobs=-1)\n",
    "\n",
    "pipe = imbpipeline(steps=[[\"feature_selection\",  SelectFromModel(estimator=feat_sel, threshold=\"median\")],\n",
    "                          [\"ros\", ros], [\"scaler\", scaler], [\"model\", model]])\n",
    "\n",
    "train = TrainModel(X_train, y_train, X_test, y_test, X_val, y_val, model=model, pipeline =pipe, pipeline_params=pipeline_params)\n",
    "train.train_with_validation(use_pipeline=True,  use_grid_search=False) \n",
    "train.evaluate_model()\n",
    "train.selected_features()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM with XgBoost Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= SVC(random_state=42,probability=True, max_iter= 1000)\n",
    "feat_sel = xgb.XGBClassifier(objective =\"binary:logistic\", enable_categorical=True, random_state = 42, n_jobs =-1)\n",
    "\n",
    "pipe = imbpipeline(steps=[[\"feature_selection\", SelectFromModel(estimator=feat_sel, threshold=\"median\")],\n",
    "                          [\"ros\", ros],[\"scaler\", scaler], [\"model\", model]])\n",
    "\n",
    "train = TrainModel(X_train, y_train, X_test, y_test, X_val, y_val, model=model, pipeline =pipe, pipeline_params=pipeline_params)\n",
    "train.train_with_validation(use_pipeline=True, use_grid_search=False) \n",
    "train.evaluate_model()\n",
    "train.selected_features()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM with PCA Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= SVC(random_state=42, probability=True, max_iter= 1000)\n",
    "pca = KernelPCA(random_state = 42, eigen_solver = \"arpack\")\n",
    "\n",
    "\n",
    "pipeline_params = {\n",
    "    'ros': [None, ros, smote, borderline_smote, smote_nc],\n",
    "    'scaler': [ mms, scaler],\n",
    "    \"model__kernel\": [\"linear\", \"rbf\"],\n",
    "    \"model__C\": [ 10],\n",
    "    \"model__gamma\": [\"scale\"],\n",
    "    \"feature_selection__kernel\": [\"linear\", \"sigmoid\"],\n",
    "    'feature_selection__n_components': np.arange(5, 10, 1),\n",
    "}\n",
    "\n",
    "pipe = imbpipeline(steps=[[\"scaler\", scaler], [\"feature_selection\", pca], [\"ros\", ros], [\"model\", model]])\n",
    "\n",
    "\n",
    "train = TrainModel(X_train, y_train, X_test, y_test, X_val, y_val, model=model, pipeline =pipe, pipeline_params=pipeline_params)\n",
    "train.train_with_validation(use_pipeline=True, use_grid_search=False) \n",
    "train.evaluate_model()\n",
    "train.pca_components()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA-Feature-Importance\n",
    "\n",
    "- PCA but only for continuous data, if not continuous try Factor Analysis of Mixed Data (FAMD) before applying FAMD one need to normalize the data\n",
    "- Check Skewness and maybe try Yeo-Johnson transformation before standardisation.? not necessary?\n",
    "\n",
    "Since n_components is a hyperparameter, it does not learn from the data. We have to manually specify its value (tune the hyperparameter) before we run the PCA() function.\n",
    "Method 1: If your sole intention of doing PCA is for data visualization, you should select 2 or 3 principal components.\n",
    "Method 2: If you want an exact amount of variance to be kept in data after applying PCA, specify a float between 0 and 1 to the hyperparameter n_components.\n",
    "Method 3: Plot the explained variance percentage of individual components and the percentage of total variance captured by all principal components. Method 3 is the best!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_pca(X_train, scale =True, threshold=0.95):\n",
    "    # Scale the dataset; This is very important before you apply PCA\n",
    "    if scale == True:\n",
    "        sc = StandardScaler()\n",
    "        sc.fit(X_train)\n",
    "        X_std = sc.transform(X_train)\n",
    "    else:\n",
    "        print(\"Make sure you don't scale twice\")\n",
    "\n",
    "    # n_components=2 or 3 or try 0.85 for 85% of the variance in the original data, then ask for pca.n_components_ to figure out how many were used\n",
    "    pca = PCA() #Here, PCA is applied to the scaled data. The number of principal components isn't specified, so by default, it will equal the number of original features.\n",
    "    pca.fit_transform(X_std)\n",
    "    #\n",
    "    # Determine explained variance using explained_variance_ration_ attribute\n",
    "    exp_var_pca = pca.explained_variance_ratio_\n",
    "    #\n",
    "    # Cumulative sum of eigenvalues; This will be used to create step plot\n",
    "    # for visualizing the variance explained by each principal component.\n",
    "    #\n",
    "    cum_sum_eigenvalues = np.cumsum(exp_var_pca)\n",
    "\n",
    "\n",
    "    plt.bar(range(0,len(exp_var_pca)), exp_var_pca, alpha=0.5, align='center', label='Individual explained variance')\n",
    "    plt.step(range(0,len(cum_sum_eigenvalues)), cum_sum_eigenvalues, color= 'red', where='mid',label='Cumulative explained variance')\n",
    "\n",
    "     # Adding a horizontal line at y=0.95 (95.0% threshold)\n",
    "    if 0 < threshold < 1:\n",
    "        plt.axhline(threshold,  color='g', linestyle='dashed', label=f'{threshold*100}% threshold')\n",
    "\n",
    "    plt.ylabel('Explained variance ratio')\n",
    "    plt.xlabel('Principal component index')\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "########################\n",
    "feature_importance_pca(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_pca2(model, X_train, y_train, X_test, y_test, scale=True, pca_components=2):\n",
    "# https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance.html\n",
    "\n",
    "    if scale == True:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X_train)\n",
    "        feature_scaled_train = scaler.transform(X_train)\n",
    "        feature_scaled_test = scaler.transform(X_test)\n",
    "\n",
    "    else:\n",
    "        print(\"Make sure you don't scale twice\")\n",
    "  \n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=pca_components)\n",
    "    X_train_pc = pca.fit_transform(feature_scaled_train)\n",
    "    X_test_pc = pca.transform(feature_scaled_test)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train_pc, y_train)\n",
    "    # Predict using the model\n",
    "    y_pred = model.predict(X_test_pc)\n",
    "\n",
    "    # Calculate and print precision and accuracy\n",
    "    p = precision_score(y_test, y_pred)\n",
    "    a = accuracy_score(y_test, y_pred)\n",
    "    print('Precision Score:', p.round(4))\n",
    "    print('Accuracy Score:', a.round(4))\n",
    "\n",
    "    # Plot feature importance (assuming the model has the attribute feature_importances_)\n",
    "    #Mean decrease in impurity (MDI) \n",
    "    mdi = pd.DataFrame(model.feature_importances_, columns=['MDI'], index=['pc_{}'.format(i) for i in range(pca.n_components_)]).sort_values('MDI', ascending=False)\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.bar(mdi.index, mdi.MDI)\n",
    "    plt.title(\"Feature Importance: MDI\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot PCA loadings\n",
    "    fig, axs = plt.subplots(pca.n_components_, 1, sharex=True, figsize=(15, 10))\n",
    "    for i in range(pca.n_components_):\n",
    "        axs[i].bar(X_train.columns, pca.components_[i])\n",
    "        axs[i].set_ylabel('component '+str(i)+' loading')\n",
    "    axs[0].set_title('PCA Loadings')\n",
    "    axs[-1].set_xlabel('Features')\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Most important PC correlation\n",
    "    X_train_pc_df = pd.DataFrame(X_train_pc, columns=['pc_{}'.format(i) for i in range(pca.n_components_)], index=X_train.index)\n",
    "    corr_mi_pc = pd.concat([X_train, X_train_pc_df['pc_1']], axis=1).corr().iloc[:,-1].sort_values().to_frame()\n",
    "    print(corr_mi_pc)\n",
    "\n",
    "################################\n",
    "\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=1000, max_depth=5)\n",
    "feature_importance_pca2(model, X_train, y_train, X_test, y_test, scale=True, pca_components=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Stuff already in the class TrainModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_plot_simple(model, X_clean , y_clean, normalized=True):\n",
    "\n",
    "    model.fit(X_clean, y_clean)\n",
    "\n",
    "    if normalized == True:\n",
    "        importances = np.std([model.feature_importances_ for model in model.estimators_], axis = 0) ## or MinMaxSclaer or StandardScaler???\n",
    "    else:\n",
    "        importances= model.feature_importances_ \n",
    "\n",
    "\n",
    "    top_features = pd.Series(importances, index= X_clean.columns).sort_values(ascending=False)\n",
    "    n = len(top_features)\n",
    "    \n",
    "    # Plot feature importance of all features\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.bar(range(n), top_features[:n], align='center')\n",
    "\n",
    "    for i in range(n):\n",
    "        plt.text(i, top_features[i]+1e-4 , round(top_features[i], 3), ha='center', fontsize = 6)\n",
    "\n",
    "    plt.xlim([-1, n])\n",
    "    plt.xticks(range(n), top_features.index[:n], rotation=90)\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Feature Importance')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_importance_plot(model, X_clean, y_clean, normalized=True, threshold=0.95):\n",
    "    \"\"\"\n",
    "    Plots the feature importance and above it, plots the cumulative importance as a step function.\n",
    "    \"\"\"\n",
    "    # Compute feature importances\n",
    "    model.fit(X_clean, y_clean)\n",
    "    \n",
    "    if normalized:\n",
    "        importances = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\n",
    "    else:\n",
    "        importances = model.feature_importances_\n",
    "\n",
    "    sorted_indices = np.argsort(importances)[::-1]\n",
    "    top_features = pd.Series(importances[sorted_indices], index=X_clean.columns[sorted_indices])\n",
    "    \n",
    "    # Compute cumulative importance\n",
    "    cumulative_importances = np.cumsum(top_features.values)\n",
    "    \n",
    "    n = len(top_features)\n",
    "    \n",
    "    # Initialize the figure\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Plot feature importances as bars\n",
    "    ax1.bar(range(n), top_features.values, align='center', alpha=0.8, label='Feature Importance')\n",
    "\n",
    "    for i in range(n):\n",
    "        plt.text(i, top_features[i]+1e-4 , round(top_features[i], 3), ha='center', fontsize = 6)\n",
    "\n",
    "    ax1.set_xlabel('Features')\n",
    "    ax1.set_ylabel('Feature Importance')\n",
    "    ax1.set_xticks(range(n))\n",
    "    ax1.set_xticklabels(top_features.index, rotation=90, fontsize=8)\n",
    "    \n",
    "    # Plot cumulative importance as a step function\n",
    "    ax1.step(range(n), cumulative_importances, where=\"mid\", color=\"r\", label='Cumulative Importance')\n",
    "    \n",
    "    # Display the threshold if necessary\n",
    "    if 0 < threshold < 1:\n",
    "        ax1.hlines(threshold, xmin=0, xmax=n, colors='g', linestyles='dashed', label=f'{threshold*100}% threshold')\n",
    "        # Annotate the plot with the number of features needed for the desired cumulative importance\n",
    "        needed_num_features = np.argmax(cumulative_importances >= threshold) + 1\n",
    "        ax1.annotate(f'{needed_num_features} features', \n",
    "                     xy=(needed_num_features, threshold), \n",
    "                     xytext=(needed_num_features, threshold+0.05),\n",
    "                     arrowprops=dict(facecolor='black', arrowstyle='->'),\n",
    "                     fontsize=9,\n",
    "                     color='g')\n",
    "    \n",
    "    # Add legend to show both types of importance\n",
    "    ax1.legend(loc=\"upper left\")\n",
    "    ax1.set_title(f\"{model}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "tree = ExtraTreesClassifier(random_state=42)\n",
    "combined_importance_plot(tree, X_train, y_train, normalized=False, threshold=0.95)\n",
    "\n",
    "forest = RandomForestClassifier(random_state = 42)\n",
    "combined_importance_plot(forest, X_train, y_train, normalized=False, threshold=0.95)\n",
    "\n",
    "xgbc = xgb.XGBClassifier(random_state = 42, enable_categorical=True)\n",
    "combined_importance_plot(xgbc, X_train, y_train, normalized=False, threshold=0.95)\n",
    "\n",
    "decision = DecisionTreeClassifier(random_state=42)\n",
    "combined_importance_plot(decision, X_train, y_train, normalized=False, threshold=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catboost_feature_importance(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    categorical_features = X_train.select_dtypes(include='category').columns.tolist()\n",
    "    ## Convert to int or string\n",
    "    for cat in categorical_features:\n",
    "        X_train[cat] = X_train[cat].astype('string')\n",
    "        X_test[cat] = X_test[cat].astype('string') \n",
    "      \n",
    "    # Create a CatBoostClassifier\n",
    "    model = CatBoostClassifier(iterations=1000, \n",
    "        learning_rate=0.05, \n",
    "        depth=6,\n",
    "        cat_features=categorical_features,\n",
    "        random_state = 42,\n",
    "        verbose=200)\n",
    "    \n",
    "    # Fit the classifier on the data\n",
    "    model.fit(X_train, y_train, cat_features=categorical_features, verbose=0)\n",
    "    \n",
    "    # Get the feature importances\n",
    "    importances = model.feature_importances_\n",
    "    \n",
    "    # Create a DataFrame to store the feature importances\n",
    "    feature_importances = pd.DataFrame({'Feature': X_train.columns, 'Importance': importances})\n",
    "    \n",
    "    # Sort the features based on importance\n",
    "    feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Select the top 17 features\n",
    "    feature_importances = feature_importances.head(17)\n",
    "    \n",
    "    # Plot the feature importances\n",
    "    plt.figure(figsize=(12, 8))  # Adjusted size for better visibility of annotations\n",
    "    bars = plt.barh(feature_importances['Feature'], feature_importances['Importance'], align='center')\n",
    "    \n",
    "    for bar in bars:\n",
    "        plt.text(bar.get_width() - (0.02 * max(importances)),  # Adjust 0.02 for distance from the end of bar\n",
    "                 bar.get_y() + bar.get_height()/2,\n",
    "                 f'{bar.get_width():.2f}',  # Formatting to 2 decimal places\n",
    "                 ha='center', va='center',\n",
    "                 color='black')\n",
    "    \n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.title('Feature Importance using CatBoost')\n",
    "    plt.gca().invert_yaxis()  # This will display the most important feature at the top\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "catboost_feature_importance(X_train.copy(deep=True), y_train.copy(deep=True) , X_test.copy(deep=True) , y_test.copy(deep=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_feature_importance(model, X_train, y_train):\n",
    "\n",
    "    # Fit the classifier on the data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Calculate permutation importances\n",
    "    results = permutation_importance(model, X_train, y_train, scoring='accuracy', random_state=42)\n",
    "    # Get the feature importances\n",
    "    importances = results.importances_mean\n",
    "    # Create a DataFrame to store the feature importances\n",
    "    feature_importances = pd.DataFrame({'Feature': X_train.columns, 'Importance': importances})\n",
    "    # Sort the features based on importance\n",
    "    feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n",
    "    # Print the feature importances\n",
    "    print(feature_importances)\n",
    "\n",
    "    # Select the top 17 features\n",
    "    feature_importances = feature_importances.head(17)\n",
    "    # Plot the feature importances\n",
    "    plt.figure(figsize=(12, 8))  # Adjusted size for better visibility of annotations\n",
    "    bars = plt.barh(feature_importances['Feature'], feature_importances['Importance'], align='center')\n",
    "    \n",
    "    for bar in bars:\n",
    "        plt.text(bar.get_width() - (0.02 * max(importances)),  # Adjust 0.02 for distance from the end of bar\n",
    "                 bar.get_y() + bar.get_height()/2,\n",
    "                 f'{bar.get_width():.2f}',  # Formatting to 2 decimal places\n",
    "                 ha='center', va='center',\n",
    "                 color='black')\n",
    "    \n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.title('Feature Importance using Permutation Importance for ????? Classificer')\n",
    "    plt.gca().invert_yaxis()  # This will display the most important feature at the top\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "model = DecisionTreeClassifier(random_state = 42)\n",
    "permutation_feature_importance(model, X_train.copy(deep=True), y_train.copy(deep=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier for each year\n",
    "\n",
    "### Accuracy with random prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_per_year():\n",
    "  # Initializing dataset path\n",
    "  path = os.path.join(sys.path[0])\n",
    "  # Loading and Handling dataset\n",
    "  mydata_new = OurDataset(path)\n",
    "\n",
    "  ### Removing and sorting stuff\n",
    "  mydata_new.remove_timestamps()\n",
    "  mydata_new.remove_duplicates()\n",
    "  mydata_new.sort_after_date()\n",
    "  mydata_new.remove_columns(columns = ['Financial_Ratio_11', 'Financial_Ratio_14'])\n",
    "  \n",
    "  years = mydata_new.list_years()\n",
    "  years.pop(0) ## remove first year, not sufficient data\n",
    "\n",
    "  for index, year in enumerate(years):\n",
    "        print(\"\\n\\n\")\n",
    "        print(f\" -------------{year}--------------- \\n\")\n",
    "        year_df = mydata_new.get_year_dataset(year)\n",
    "        year_df.split_dataset(imputer=\"iterative\", remove_outliers=True) # drop_all will not work here!\n",
    "        X_train, X_test, X_val, y_train, y_test, y_val = year_df.X_train, year_df.X_test, year_df.X_val, year_df.y_train, year_df.y_test, year_df.y_val\n",
    "\n",
    "        model = RandomForestClassifier(random_state = 42)\n",
    "        pipeline_params = {\n",
    "          \"model__criterion\": [\"gini\", \"entropy\"],\n",
    "          \"model__max_features\": [ \"sqrt\", \"log2\"],\n",
    "          \"model__max_depth\": np.array([None, 5, 10]),\n",
    "          \"model__n_estimators\": np.array([ 1, 5, 10, 50, 100]),\n",
    "          \"model__class_weight\": [None, \"balanced\", \"balanced_subsample\"],\n",
    "          }\n",
    "        pipe = imbpipeline(steps=[[\"ros\", ros], [\"model\", model]])\n",
    "        train = TrainModel(X_train, y_train, X_test, y_test, X_val, y_val, model=model, pipeline=pipe, pipeline_params=pipeline_params)\n",
    "        print(f\"Random Forrest Classifier trained on data from {year}\\n\")\n",
    "        train.train_with_validation()\n",
    "        train.evaluate_model()\n",
    "        plt.show()\n",
    "        print(\"\\n\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "random_forest_per_year()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import glob\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set random seed to ensure reproducibility\n",
    "torch.manual_seed(0xbeef)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(sys.path[0])\n",
    "mydata_nn = OurDataset(path)\n",
    "\n",
    "### Removing and sorting stuff\n",
    "mydata_nn.remove_timestamps()\n",
    "mydata_nn.remove_duplicates()\n",
    "mydata_nn.sort_after_date() \n",
    "\n",
    "\n",
    "\n",
    "## Imputing Missing Values and Outliers\n",
    "# if dropping all missing columns be sure to drop the column 'Financial_Ratio_11? since we only got data for 2022 and 2023.\n",
    "mydata_nn.remove_columns(columns = ['Financial_Ratio_11', 'Financial_Ratio_14']) \n",
    "mydata_nn.split_dataset(imputer = \"iterative\", remove_outliers=True)\n",
    "X_train, X_test, y_train, y_test = mydata_nn.X_train, mydata_nn.X_test, mydata_nn.y_train.values, mydata_nn.y_test.values\n",
    "\n",
    "scaler = MinMaxScaler() \n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "X_train_std, X_val_std, y_train, y_val = train_test_split(X_train_std, y_train, stratify = y_train, test_size = 0.2, random_state = 42) ### Train and Validation Set!\n",
    "\n",
    "y_test = y_test.to_numpy()\n",
    "y_val = y_val.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "\n",
    "print(type(y_test))\n",
    "print(type(X_train_std))\n",
    "print(type(X_test_std))\n",
    "print(type(X_val_std))\n",
    "print(type(y_val))\n",
    "print(type(y_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataSet(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X.astype('float')\n",
    "        self.y = y.astype('float')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # provide row `idx` from X and y\n",
    "        return torch.from_numpy(self.X[idx, :]).type(torch.FloatTensor), torch.tensor(self.y[idx]).type(torch.FloatTensor)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = CustomDataSet(X_train_std, y_train)\n",
    "testset  = CustomDataSet(X_test_std, y_test)\n",
    "valset = CustomDataSet(X_val_std, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "trainloader = DataLoader(trainset, batch_size = batch_size, shuffle = True)\n",
    "testloader = DataLoader(testset, batch_size = batch_size, shuffle = False)\n",
    "valloader= DataLoader(valset, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNeuralNet(nn.Module):\n",
    "    def __init__(self, input_dimension):\n",
    "        super(CustomNeuralNet, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_dimension, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 1)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.classifier(x)\n",
    "        return self.sigmoid(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    loss_vals = []\n",
    "    correct = 0.0\n",
    "    for _, (X, y) in enumerate(dataloader):\n",
    "        pred = model(X)\n",
    "        y = y.type(torch.float).unsqueeze(1)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_vals.append(loss.item())\n",
    "\n",
    "        # binary classification, round to get the most likely result\n",
    "        correct += (torch.round(pred) == y).type(torch.float).sum().item()\n",
    "    return correct/size, sum(loss_vals)/len(loss_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(dataloader, model, loss_function):\n",
    "    size = len(dataloader.dataset)\n",
    "    correct = 0.0\n",
    "    loss_vals = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            y = y.type(torch.float).unsqueeze(1)\n",
    "            loss_vals.append(loss_function(pred, y).item())\n",
    "            # binary classification, round to get the most likely result\n",
    "            correct += (torch.round(pred) == y).type(torch.float).sum().item()\n",
    "    return correct/size, sum(loss_vals)/len(loss_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(model, trainloader, valloader, loss_function, optimizer, epochs=5, model_path='./model.pt'):\n",
    "    best_accuracy = 0.0\n",
    "    best_loss = 1.0\n",
    "    consecutive_worse = 0\n",
    "    for t in range(epochs):\n",
    "        print(f\"================ Epoch {t} ================\\n\")\n",
    "        train_accuracy, train_loss = train(trainloader, model, loss_function, optimizer)\n",
    "        print(f\"Train Accuracy: {(100*train_accuracy):>0.1f}%, Train Loss: {train_loss:>8f} \\n\")\n",
    "        val_accuracy, val_loss = validate(valloader, model, loss_function)\n",
    "        print(f\"Validation Accuracy: {(100*val_accuracy):>0.1f}%, Validation Loss: {val_loss:>8f} \\n\")\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            best_loss = val_loss\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            consecutive_worse = 0\n",
    "        else:\n",
    "            consecutive_worse += 1\n",
    "        if consecutive_worse >= 5:\n",
    "            print(f\"Early stop epoch {t}\")\n",
    "            break\n",
    "    print(f\"Done! Best Model Accuracy: {(100*best_accuracy):>0.1f}, Best Model loss: {best_loss}\")\n",
    "    # Load the best model\n",
    "    model = CustomNeuralNet(len(X_train_std[0]))\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomNeuralNet(len(X_train_std[0]))\n",
    "#print(model)\n",
    "best_model_path = './custom_nn_model.pt'\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "trained_model = run_training(model, trainloader, valloader, loss_function, optimizer, model_path=best_model_path, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the model on the test data\n",
    "test_accuracy, _ = validate(testloader, model, loss_function)\n",
    "print(f\"Test Accuracy: {(100*test_accuracy):>0.1f}%\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
